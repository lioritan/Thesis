Questions
1. [Summary] Please summarize the main claims/contributions of the paper in your own words.
The authors present a method for generating features from a knowledge base to be used to enrich the input data in classification. The approach is evaluated on a text classification task on a variety of corpora.

The paper reads well and is not overly technical. The proposed approach seems to be rather simple and to provide a reasonable way to harness knowledge from KBs. However, it's unclear how much effort must go, in practice into cleaning and filtering, in order to ensure a good quality of produced feature and avoid generating garbage useless features.

It would be nice to include more in depth discussion of various aspects such as the limitations of the method, of the increased complexity, and the impact on interpretability and risk of overfitting.

The experiments need strengthening, they have been carried out on a quite large number of corpora, but mostly from the same source and with the same task.

To argue that the method is general, the experiments should cover a more diverse range of tasks and domains. Also in the case of text classification the proposed approach comes as one piece in a complex text processing pipeline, making it difficult to untangle the effect of the proposed method from the other elements. That is, when the method underperforms compared to expectations, it's difficult to know why. So experiments on something where the features are generated and used in a more direct fashion would be quite enlightening. 

Furthermore, there has been research in feature generation for quite some years, so the proposed approach needs to be put more clearly in perspective, also in experiments.
2. [Relevance] Is this paper relevant to an AI audience?
Relevant to researchers in subareas only
3. [Significance] Are the results significant?
Significant
4. [Novelty] Are the problems or approaches novel?
Somewhat novel or somewhat incremental
5. [Soundness] Is the paper technically sound?
Technically sound
6. [Evaluation] Are claims well-supported by theoretical analysis or experimental results?
Somewhat weak
7. [Clarity] Is the paper well-organized and clearly written?
Good
8. [Detailed Comments] Please elaborate on your assessments and provide constructive feedback.
p.1: Fürmkranz : Fürnkranz (RN, not M)
p.1: set of example: examples
p.2: Assuming S \tilde D : clarify what you mean by that
p.4: To label each... : why separate the case when there is a single value vs when there are several? Taking the majority over a single value will yield that value, i.e. the behaviour is the same
p.5: This can be a result of mistakes...: it would be good to be able to say something more definitive about this
p.5: As we have discussed in section ,: section number missing
p.5: the selected learning algorithms are not exactly cutting edge, isn't there any more recent method that could be used?
p.6: In the example task, if I understand correctly, the aim is to classify pieces of text depending on whether they refer to locations in on around either New York or Texas. It seems the first layer of feature generation is doing just that by extracting location information and checking whether it it related to NY or TX. I don't quite understand what the role of the second layer is, i.e. what is the goal of checking whether the location is related to some battle or conflict. Can you provide some numbers about the number of cases where the first layer (Texas) is useful in producing the correct class, vs. the second layer (conflict/war)?
In this example, there is first a test of whether the location is related to Texas, then whether it is related to conflict of battle, but there is no actual test of whether the location is related to NY, is there a reason for that?
From the last sentence in the paragraph, this is a practical example of feature generated by the algorithm. Maybe make that clearer upfront.
Related to this is the question of how easily understandable and interpretable are the generated features. How about the risk of overfitting?
p.6: It seems to me that the example with text classification involves a long and complex pipeline, with extraction and matching of entities, etc. Isn't there a more direct task, that would allow to evaluate the feature generation process with less risk of dilution of the effects along the pipeline?
p.7: based on an a given: based on a given
p.7: why not compare to combinational approaches in the experiments, eg. deep learning methods to have a base of comparison on classification performance?
p.7: Propositionalization approaches (2000; 2011): the author names are missing
p.7: Cheng et al.: ref year missing
p.7: To that end: Rather "to circumvent/solve this issue" (also earlier in the paper)
p.7: it requires features with meaningful values: what do you mean by that?
How about the runtime? I.e. how costly is the generation of features in the experimental scenarios presented?
9. [QUESTIONS FOR THE AUTHORS] Please provide questions for authors to address during the author feedback period.
Can you discuss a bit more about the possible risks and shortcomings of the approach?
10. [OVERALL SCORE]
6 - Marginally above threshold
11. [CONFIDENCE]
Reviewer made an educated guess