\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

%my stuff
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage[titletoc,toc,title]{appendix}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\begin{document}
\pdfinfo{
/Title (Feature Generation by Recursive Induction)
/Author (Lior Friedman, Prof. Shaul Markovitch)
/Keywords (Machine Learning, Open Linked Data, Feature Generation, Text Categorization)
}

\title{Feature Generation by Recursive Induction}

\author{Lior Friedman \and Prof. Shaul Markovitch\\
Technion-Israel Institute of Technology, xxx, Haifa\\
liorf@cs.technion.ac.il  \\ shaulm@cs.technion.ac.il
}

\maketitle
\begin{abstract}
\begin{quote}
Induction algorithms have steadily improved over the years, resulting in powerful methods for learning. However, these methods are constrained to use knowledge within the supplied feature vectors. Recently, a large collection of common-sense, domain specific knowledge bases have become available on the web. The natural question is how these knowledge bases can be exploited by existing induction algorithms.
In this work we propose a novel algorithm for using relational data to generate recursive features. Given a feature, the algorithm recursively defines a new learning task over its set of values, and uses the relational data to construct the feature vectors. The resulting classifier is then added as a new feature.
We have applied our algorithm to the domain of text categorization, using large semantic knowledge bases such as YAGO. We have shown that generated recursive features significantly improve the performance of existing induction algorithms.
\end{quote}
\end{abstract}

\noindent %insert intro here  %TODO+ all citet/citep -> cite

\label{sec:Intro}
%present the problem, where it matters
In recent decades, we have seen an increasing prevalence of machine learning techniques used in a wide variety of fields such as medical diagnosis, vision, and biology.
Most machine learning methods assume a given set of labeled examples represented by a set of
pre-defined features. These methods have proven to be successful when a collection of good,
distinguishing features is available.
In many real-world applications, however, the given set of features is not sufficient for inducing a high quality classifier.

One approach for overcoming the difficulty resulting from an insufficiently expressive set of features, is to generate new features.  Most feature generation algorithm produce new features by combining the original ones in various ways.  For example, the LFC algorithm \cite{ragavan1993complex} combines the original feature using logical operators.  The LDMT algorithm \cite{utgo1991linear} use linear combinations of the original features to construct more informative ones.  The FICUS algorithm \cite{markovitch2002feature} presents a general framework for using any set of constructors to combine features.

While feature combination has proven to enhance the performance of induction algorithms, there are many cases where a mere combination of existing features is not enough.  To that extent, a new approach for generating features has been devised.  This approach aims to incorporate additional knowledge from external sources to construct new and informative features.
\cite{gabrilovich2007computing} for example, presents a method for generating features that are based on Wikipedia concepts.  \cite{jarmasz2004roget} presents a method for utilizing lexical links between words to generate features.

%Paragraph talking about FG when the external knowledge is organized as a set of relations.
%Description of existing methods.
%Saying that they are good start - but do not exploit the deep structure or some other deficiency (must lead to our approach )
In the case where this external knowledge is organized as a set of relations between entities, several %TODO

In this work, we present a new methodology for using relational knowledge bases for feature construction.  Our algorithm uses common induction algorithms over sets of feature values, resulting in classifiers which are then used as new features.
Such classifier-based features can effectively capture the complex relationships that are difficult to automatically discover using existing feature-generation methods.

As an example of the potential use of such an approach, consider the following:
Suppose we are given medical records of patients containing name, country of origin as well as medically relevant information.
Our task is to identify patients with high risk of developing a genetic disease, which is more common in warm countries as well as countries with a lower GDP.
The original set of features is not sufficient for inducing the target concept.  Assume, however, that we have access to an external knowledge base, containing, among others, relations specifying information about countries and connecting last names with country of origin.
Our method would first formulate a new induction problem where the positive examples are last names of high-risk patients, and then, through a recursive process, formulate a new problem where the positive examples are countries of origin for last names of such high risk patients. The resulting classifier could then be used as a regular binary feature when classifying patients, allowing us to capture the genetic component of the disease through the last name.
While traditional feature generation methods could XXXXXX, they would struggle to find the above relationship, as it is too complex.

%%%%%%%%%%%%%TODO%%%%%%%%%
%insert example drawing here
%%%%%%%%%%%%%TODO%%%%%%%%%

While our proposed method is general and can be used for any induction task, in this work we will focus on applying the method for the important task of text classification. In recent years very large relational databases have developed significantly and used as knowledge bases in several tasks. We will show how our method can exploit these knowledge bases for enhancing the performance of text classifiers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background} \label{background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
One of the earliest methods of making use of additional information, suggested by \cite{muggleton1991inductive} and used in \cite{quinlan1990learning}, is \emph{Inductive Logic Programming(ILP)}. This approach induces a set of first-order formulae
that induces a good separation on the given training set. Originally, ILP methods searched the space of first-order formulae to find the appropriate classifier.
Over time, effort was made to allow the usage of traditional propositional algorithms such as ID3\cite{quinlan1986} over this space.

One major attempt at adding knowledge to traditional methods was \emph{propositionalization}\cite{kramer2000bottom}: Since we wish to allow use of first-order predicates in propositional methods, we create all possible(non recursive) first-order predicates and use them as features.
A major setback of this process is that it generates an impractically large number of features, most of which irrelevant.  To this end, \emph{upgrade} methods such as \cite{van2001upgrade} were suggested, where instead of a-priori creating all predicates, we search over predicates in a refinement process, guided by the labeled training data.

As a continuation to this trend, \cite{popescul2003statistical} suggested SLR, an upgrade method which allows the generation of non-binary attributes by using aggregation operators. Essentially, instead of simply checking whether a first-order predicate is satisfied, we perform aggregation over all objects which satisfy it. In addition, SRL offers an initial insight into the issue of search by using Akaike's information criteria(AIC, see \cite{burnham2002model}) to select features based on complexity as well as predictive power.

Newer advances in the relational approach can be seen in \emph{Statistical Relational Learning} \cite{blockeel2013statistical,nath2014learning} as well as \emph{Collective Classification} \cite{kajdanowicz2013collective,laorden2012collective}. Collective classification attempts to label a group of objects together, and both approaches place a much stronger emphasis on data structure, making them less relevant to the discussed setting.

%insert generic feature generation background(shaul+rozenstien)

On the other end of the spectrum, most Feature Generation techniques are designed as a local search over the space of constructed features.
\cite{markovitch2002feature} proposed a general framework for feature generation where a set of constructor functions  with associated grammar is given as input.  The algorithms then uses k-beam local search over the space of constructed features.

A good example of the potential power of adding additional knowledge to learning tasks can be found within the domain of text classification and categorization, where \cite{gabrilovich2007computing}
introduced the \emph{explicit sematic analysis} method for generating concept-based features based on knowledge sources such as Wikipedia.
This use of semantics allowed for a richer representation of the text than the existing Bag-of-Words model and thus achieving an increase in accuracy.

Recently, there have been several efforts in utilizing Linked data for Feature Generation \cite{cheng2011automated,paulheim2012unsupervised}. While these techniques offer some insight towards applying the knowledge within such an extensive knowledge base as features, they often add them in an unsupervised manner, leading to the creation of an impractically large number of features, most of which are irrelevant, as seen in propositionalization methods. Due to this, they have difficulties with complex relationships and must limit search.

A well known and well explored task within the field of Natural Language Processing(NLP) is that of text classification.
The task of text classification is that of assigning categories or labels to documents, based on their content. The potential labels/categories are given in advance, and we have a collection of labeled documents to use as a training set. This task has many practical applications, including spam filtering as well as identifying genre, sentiment, or topic.

Until recent years, text classification systems represented text almost exclusively as a \emph{Bag of Words}, thus creating a vector space representation\cite{salton1983introduction,Wu:1981:CST:1013228.511759}. While this method offered simplicity, it had inherent limitations in terms of representation power.

A major breakthrough came in the form of \cite{gabrilovich2006overcoming}, which used semantic concepts extracted from knowledge sources such as Wikipedia as features. This technique allowed for a richer representation of the text, and has shown improvement over the Bag-of-Words representation for the task of text classification, especially in shorter texts.

As mentioned before, the rising popularity of Semantic Linked data such as YAGO2\cite{hoffart2013yago2} and Open Linked Data\cite{bizer2009linked} have led to the creation of new approaches to make use of this powerful, ontology based representation of knowledge\cite{bloehdorn2007kernel,rios2014statistical}.
We also intend to apply our method to this domain, as our method seems especially well suited to build make use of such ontology based knowledge bases by adding it into the learning process in an effective manner.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%explain how gonna solve
Let $O$ be a set of objects. Let $Y=\{0,1\}$ be a set of labels (we assume binary labels for ease of discussion). Let $S=\{(o_{1},y_{1}),\ldots,(o_{m},y_{m})\}$ be a set of labeled examples such that $o_{i}\in O, y_{i}\in Y$. Let $F=\{f_{1},\ldots,f_{n}\}$ be a \emph{feature map}, a set of \emph{feature functions} $f_{i}:O\rightarrow Dom_{i}$.  This definition implies a training set represented be feature vectors: $\{ (\langle f_1(o_i),\ldots,f_n(o_i)\rangle, y_i) | (o_i,y_i) \in S\}$.
%Let ${\cal H}_{F}$ be a set of hypotheses defined over feature map $F$.

%%% Given a set of relations (not binary, assuming wlg that the fist argument is
%%% the key.  For each relation R with n arguments We define a new set of n-1
%%% binary relations, where each of the first elements is the key. Name it
%%% mention that the new binary relations are actually functions

Given a set of relations $\bar{R}=\{R_{1},\ldots,R_{t}\}$ with arity of $n_{j}$ ($j=1\ldots k$), we can assume \text{w.l.g} that the first argument is a key. For each relation $R_{j}$ we define $n_{j}-1$ new binary relations where each of the first elements is the key and the second one is another column.
Let ${\cal R}=\{R_{1},\ldots,R_{k}\}$ be such set of binary relations, where $R_{i}$ is defined over $K_{i}\times D_{i}$. These relations can thus be seen as functions $R_{i}: K_{i}\rightarrow D_{i}$.

\begin{defn}
A \emph{feature generation algorithm} $A$ using relations is an algorithm that given $\langle S,F,{\cal R} \rangle$, creates a new feature map $F_{\cal R}=\{f'_{1},\ldots,f'_{l}\}$.
\end{defn}

We would like the new hypothesis space, defined over $F_{\cal R}$, to be one that is both rich enough to provide hypotheses with a lower loss than those in the original one, as well as simple enough that the learning algorithm used will be able to find such good hypotheses given training data.
We would also like there to be connections between $F$ and ${\cal R}$, meaning some of the feature values of features in $F$ (when applied to objects in $S$) also exist within relations in $\cal R$.

We propose a general feature generation algorithm using additional knowledge on feature values.
Given an original feature $f_{i}$, with domain $Dom_i$, our algorithm will formulate a new learning task trying to separate values in $Dom_i$ appearing in positive examples of the original learning task from those appearing in negative ones.  The result of the new learning task will be a classifier
$h_{i}:Dom_{i}\rightarrow \{0,1\}$ that can label feature values of $f_{i}$. We can then define a new binary feature $f'_{i}(x)=h_{i}(f_{i}(x))$.

%%%%%%%%%%%%%%%%TODO: from here
In order to explain how we create such $h_{i}$, let us consider a single step of the feature generation algorithm.
Given a feature $f_{i}$, we define the set of all its value in the training set $S$ as $v_i(S) = \{v | (o,y) \in S, f_{i}(o)=v\}$. In the intro example, for instance, $v_i(S)$ can be the set of all last names of patients.
We now formulate a new learning problem with the new training set
$\hat{S_i} = \{ (v, label(v)) | v \in v_i(S) \}$.
The labeling function can be, for example, defined as
the majority label: $label(v)=majority(\{y_k| \left(o_k,y_k \right) \in S, f_{i}(o_k)=v\})$.

To define a feature map over the new training set $\hat{S}$, we look for all the relations in $\cal R$ where the domain of the key argument contains $v_i$:
${\cal G}(S,{\cal R}) = \left\{ r \in {\cal R} | v_i(S) \subseteq \{x_1 | (x_1,x_2) \in r\}\right\}$.  In the intro example, one such $r$ can be a relation mapping last names to countries of origin.
Solving this new learning problem yields our classifier $h_{i}$.
Note that during the process of learning $h_{i}$, we can once again call the feature generation procedure to generate useful features for \emph{that} task, hence the recursive aspect of the process.

%TTODO: tree stuff instead of search, then RFG explain and then hirarchic
Through the above process, we can define a tree of possible domains to search over as follows:
\begin{defn}%defined recursivly. add that we can do ANY transform G using S,R,F
$S$ is a domain.\\
If $\hat{S}$ is a domain, then for any transform $G_{\hat{F},\hat{R}}$, $\{(v_i,label(v_i)|(o,y)\in \hat{S}, v_i=G_{\hat{F},\hat{R}}(o)\}$ is also a domain.
\end{defn}
Given sufficient resources, we could potentially search this tree exhaustively during the learning process. In most practical cases, however, an exhaustive search proves intractable, and we must select a search strategy over this tree.

%In order to discuss the feature generation and selection process more comprehensively, we must first define a search space.
%We define our search space as a trio $\langle\hat{S},\hat{F},\hat{R}\rangle$, where $\hat{S}$ is a set of labeled examples-pairs $(\hat{o},\hat{y})\in (\hat{O},\{0,1\})$, $\hat{F}$ is a feature map from $\hat{O}$ to $Dom_{1},\ldots,Dom_{\hat{n}}$ and $\hat{R}$ is a set of relations.
%Our initial state is $\langle S,F,{\cal R}\rangle$. In each step, we can choose to either stop our search and choose a classifier on $\hat{O}$ as discussed above, or we can choose to continue the process in one of the methods below.

Now, we will discuss \emph{Basic-RFG} (See full algorithm in appendix \ref{code2}):
The basic version opts for transformations of the following form: given $\hat{S},\hat{F},\hat{R}$ and a feature $f_{j}\in \hat{F}$, $G_{i}(o)=R_{i}(f_{j}(o))$ ($R_{i}\in \hat{R},Dom_{j}\cap K_{i}\neq \emptyset$).
The result is a process that bears some similarity to that of search over a refinement graph\cite{dvzeroski2001introduction,van1998completeness}, a common technique in ILP.

The unique contributions of this method are as follows:
\begin{itemize}
    \item By moving our domain space when constructing a new problem, we essentially look at the problem from another perspective, which can discover connections which may not be immediately apparent, as well as allow for discovery of complex relationships.
    \item We can use a traditional learning algorithm we are comfortable with when we create a classifier, and possibly use different ones as we make recursive steps.
\end{itemize}

We can also consider a \emph{local} variation for the algorithm, where instead of finding features that perform well on the entirety of the data, we find features which are good on specific subsets. We use one of the above methods to find a single binary feature, and apply it to the objects. Now, we split the objects to those labeled positive and those labeled negative, and repeat the process as we wish.

Finally, we note that the local variation yields an algorithm that can easily be made into an \emph{anytime algorithm}: an algorithm that given more allotted computation time to learn, will produce results of higher quality\cite{zilberstein1996using}. You may see the anytime version for the case of decision trees as the classifier in appendix \ref{code3}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
None%insert evaluation detail+results
%In order to measure the performance and significance of this approach, we intend to empirically evaluate its performance for the domain of document classification. We intend to look at several datasets, ranging from classical, often used datasets such as 20 newsgroups \citep{Lang95} and OHSUMED \citep{hersh1994ohsumed} to more modern ones such as Reuters Corpus Volume I(RCV1)\citep{lewis2004rcv1} and TechTC \citep{davidov2004parameterized}. For Semantic linked data, we first intend to use YAGO2(\citet{hoffart2013yago2}). We will later consider the usage of the Open Linked Data project(\citet{bizer2009linked}) and conceptNet5(\citet{speer2012representing}).\\
%
%We intend to compare the accuracy of our approach against the achievable accuracy without the use of feature generation, as well as the accuracy achievable by other feature generation methods such as \citet{gabrilovich2006overcoming}.
%
%We will place focus on decision trees and the basic, local algorithm, an instance which allows easier adaptation as well as interpretability.
%In addition, we intend on measuring the anytime performance of its anytime variation compared to traditional anytime algorithms that do not make use of additional feature generation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concluding Remarks and Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
stuff

\bibliography{document}
\bibliographystyle{aaai}

%\appendix
\begin{appendices}
\pagebreak

%\section{Basic-RFG algorithm} \label{app:1}
%\begin{algorithm}[H]
%$S=\{(o_{i},y_{i})\}$- set of labeled objects. We will mark $Ob$ the objects and $y$ the appropriate labels.
%
%$F$- A set of feature-functions.
%
%$\cal R$- A set of relations.
%
%$G$- A transformation function.
%
%$n$- Number of features to generate.
%
%$cond$- Stopping criteria.
%
%classifiers- An array of the classifier to be used at every level of the recursive process.
%
%\caption{Global-RFG}
%\label{code1}
%\begin{algorithmic}
%\Function{RFG}{S, F, R, G, n, classifiers, cond}
%\If {cond(S,F) or n $< 1$}
%    \State
%    \Return \Call {classifiers[0]}{F(Ob),y}
%\EndIf
%\State featureChosen= \Call {pickFeature}{S,F, n}
%\State newObjects= featureChosen(Ob)
%\State newLabels= \Call {label}{S,newObjects}
%\State newF= \Call {G}{S,F,R}
%\State newClassifier= \Call {RFG}{(newObjects,newLabels), newF, R, G, 1, classifeirs[1:], cond}
%\State newFeature= function(x): return \Call {newClassifier}{featureChosen(x)}
%\State
%\Return newFeature$\cup$ \Call {RFG}{S, F, R, G, n-1, classifiers, cond}
%\EndFunction
%
%\end{algorithmic}
%\end{algorithm}

\section{Basic-RFG algorithm} \label{app:2}
\begin{algorithm}[H]
$S=\{(o_{i},y_{i})\}$- set of labeled objects. We will mark $Ob$ the objects and $y$ the appropriate labels.

$F$- A set of feature-functions.

$\cal R$- A set of relations.

$n$- Number of features to generate.

$cond$- Stopping criteria.

classifiers- An array of the classifier to be used at every level of the recursive process.

\caption{Basic-RFG}
\label{code2}
\begin{algorithmic}
\Function{B-RFG}{S, F, R, n, classifiers, cond}
\If {cond(S,F) or n $< 1$}
    \State
    \Return \Call {classifiers[0]}{F(Ob),y}
\EndIf
\State featureChosen= \Call {pickFeature}{S,F, n}
\State newObjects= featureChosen(Ob)
\State newLabels= \Call {label}{S,newObjects}
\State newF= $\{R_{i}(newObjects)|R_{i}\in R, newObjects\cap D_{i}\neq\emptyset\}$
\State newClassifier= \Call {B-RFG}{(newObjects,newLabels), newF, R, 1, classifeirs[1:], cond}
\State newFeature= function(x): return \Call {newClassifier}{featureChosen(x)}
\State
\Return newFeature$\cup$ \Call {B-RFG}{S, F, R, n-1, classifiers, cond}
\EndFunction

\end{algorithmic}
\end{algorithm}

\section{Anytime Local-Hierarchy-RFG Algorithm} \label{app:3}
\begin{algorithm}[H]
\caption{Anytime Tree-RFG}
\label{code3}
\begin{algorithmic}
\Function{Anytime-Improved-Tree}{currentTree, S, F, R, cond}
\If {timeout}
    \State
    \Return currentTree
\EndIf
\State nodeToImprove= \Call {pickNode} {currentTree, S}
\State newClassifier= \Call {B-RFG} {nodeToImprove.S, F, R, 1, [decisionTree], cond}
\State newFeature= function(x): return \Call {newClassifier}{featureChosen(x)}
\State posSet= $\{(x,\_)\in$ nodeToImprove.S$|$newFeature(x)==1$\}$
\State negSet= S-posSet
\State newNode= \Call {makeNode} {newFeature, nodeToImprove.S, posSet, negSet}
\State currentTree.replace(nodeToImprove, newNode)
\State
\Return \Call {Anytime-Improved-Tree} {currentTree, S, F, R, cond}
\EndFunction

\end{algorithmic}
\end{algorithm}
\end{appendices}

\end{document} 