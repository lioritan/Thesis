\documentclass[twoside,11pt]{article}
\usepackage{jair,theapa, rawfonts}

% Use the postscript times font!
\usepackage{times}

%my stuff
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{float}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{fixltx2e}
\usepackage{dblfloatfix}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{ctable}


%mby

%\usepackage[authoryear]{natbib}
\usepackage{graphicx}

\raggedbottom %nicer enumerate
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

%\jairheading{1}{2015}{3-17}{6/91}{9/91}
\ShortHeadings{Recursive Feature Generation for Knowledge-based Induction}
{Friedman \& Markovitch}
%\firstpageno{25}

\title{Recursive Feature Generation for Knowledge-based Induction}
\author{\name Lior Friedman \email liorf@cs.technion.ac.il \\
	\name Shaul Markovitch \email shaulm@cs.technion.ac.il \\
	\addr Technion-Israel Institute of Technology\\
	Haifa 32000, Israel
	}

\begin{document}

\maketitle

\begin{abstract}
  Induction algorithms have steadily improved over the years, resulting in powerful methods for learning. However, these algorithms are constrained to using knowledge within the supplied feature vectors. In recent years, a large collection of both general and domain-specific knowledge bases have become increasingly available on the web. The natural question is how these knowledge bases can be exploited by existing induction algorithms.
  In this work we propose a novel supervised algorithm for injecting external knowledge into induction algorithms using feature generation. Given a feature, the algorithm defines a new learning task over its set of values, and uses the knowledge base to then solve the constructed learning task. The resulting classifier is then used to create new features for the original problem.
  We have applied our algorithm to the domain of text categorization, using large semantic knowledge bases such as Freebase. We have shown that generated features significantly improve the performance of existing induction algorithms.
\end{abstract}

\section{Introduction}
\label{sec:Intro}
In recent decades, we have seen an increasing prevalence of machine learning techniques used in a wide variety of fields such as medical diagnosis, vision, and biology.
Most of these methods rely on the inductive approach: given a set of labelled examples, they attempt to locate a hypothesis that is heavily supported by the known data. These methods have proven themselves successful in cases where there is a sufficient number of examples, and a collection of good,
distinguishing features is available.
In many real-world applications, however, the given set of features is not sufficient for inducing a high quality classifier.

One approach for overcoming the difficulty resulting from an insufficiently expressive set of features, is to generate new features based on existing ones. 
These feature generation approaches attempt to combine the given feature set in an effort to create better, more discriminatory features with regards to the induction problem at hand. As an example of this, the LFC algorithm \shortcite{ragavan1993complex} combines binary features through the use of logical operators such as $\land ,\lnot$.

These feature generation methods all provide us with ways to enhance the performance of induction algorithms through intelligent combinations of existing features. While this approach often suffices, there are many cases where merely combining existing features is not sufficient. 
In such cases, we require knowledge external to the problem in order to solve it effectively.
Supposed, for example, that the induction problem at hand is to identify people at risk of the genetic disorder Tay-Sachs \footnote{A recessive genetic disorder prevalent mostly among Ashkenazi Jews}. The training set contains examples of people that have developed Tay-Sachs, and examples of people that did not. Since the concept is related to a patient's country of origin, a human expert might look at surnames of former patients, and using his background knowledge regarding surnames, he would link past patient surnames to geographical locations corresponding to high risk, allowing him to identify potential patients with high accuracy.
A machine learning algorithm, on the other hand, would do poorly, as it lacks the external knowledge required to make that link.

%existing induction approaches would require a prohibitively large sample of patients to identify it from symptoms alone, as many of them can be attributed to other disorders. Even considering the personal information of a patient is clearly not enough. Yet, despite all of the above issues, a human physician can easily identify risk and signs of Tay-Sachs based on these features, even if he is given only a single example, using his domain knowledge and deductive reasoning.

One known method of utilizing external knowledge is known as Deductive Learning \cite{mitchell1982generalization,dejong1986explanation}. This school of learning methods uses a knowledge base of logical assertions in order to locate a small set of logical conditions that capture the target concept based on examples from that concept. This approach does so using deduction, rather than induction, as its main tool, allowing us to leverage a logical knowledge base effectively.
There are two main concerns that must be addressed for deductive learning to be effective:
\begin{enumerate}
	\item Extensive knowledge bases comprised of logical assertions are difficult to find and create.
	\item This approach forfeits the progress and power gained by the use of well-known and extensively researched inductive methods.
\end{enumerate}

In order to combat these issues, we can make use of the extensive relational knowledge bases that have been constructed as part of the Semantic Web project (see survey, \citeR{bizer2009linkedfull}). These knowledge bases contain type-annotated entities covering a multitude of domains that are connected via semantically meaningful relations. Thus, the question is how relational knowledge can be used to enhance existing induction methods.
There have been several efforts in utilizing Linked Data for unsupervised feature generation. \shortciteA{cheng2011automatedfull} devised a framework for constructing features from linked data. By using intelligent queries, it is possible to construct features that utilize the knowledge base in a meaningful way. 
\citeA{paulheim2012unsupervisedfull} developed FeGeLOD, an automated, fully unsupervised framework that constructs features by using entity recognition techniques to locate semantically meaningful features in the data set, and expand upon those entities using relations within the Semantic Web. %They then use feature selection techniques to remove features with a large percentage of missing, identical, or unique\footnote{Values which only appear in a single example} values.

In this work, we build upon these existing approaches in order to present a new supervised methodology for generating complex features based on a relational knowledge base. Our algorithm recursively constructs new learning problems from existing feature values, then uses knowledge bases such as the Semantic Web to construct the features for the new learning problem.
Using common induction algorithms, we can then construct a classifier that can be applied to the original problem, giving us a complex feature which takes advantage of external relational knowledge in a meaningful way.
The contributions of this approach are threefold. First, usage of training data allows for an automated, concept-driven exploration of the large space of possible features, in a manner similar to deductive approaches. Second, this approach can be applied recursively within the constructed learning problem, allowing for easier discovery of complex features. Third, it can effectively utilize one-to-many relations, which are more difficult to use in unsupervised approaches.

\section{Motivation} \label{motivation}

Before we delve into the exact description of our algorithm, we would like to illustrate its main ideas using an example.
Suppose we are attempting to identify people with a high risk of suffering from a certain genetic disease. Assume that the target concept to be discovered is that those at risk are women with ancestors originating from desert areas. To do so, we are given a training sample of sick and healthy people, containing various features, including gender and their full name. We call this learning problem $T_1$.
Assuming we have no additional information, an induction algorithm would likely produce a result similar to that shown in Figure \ref{fig:tree_base}. While such a classifier will achieve a low training error, the hundreds of seemingly unrelated surnames will cause it to generalize poorly. This phenomenon is known as over-fitting, meaning the model is too adjusted to known data and cannot generalize to previously unseen examples. We can intuitively see this in Figure \ref{fig:tree_base}, as each leaf hypothesis is supported by only a single example.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig1.pdf}
	\caption{A decision tree for the basic features}
	\label{fig:tree_base}
\end{figure}

The above example illustrates that without additional knowledge, an induction algorithm will yield a very poor result. Even if we attempt to use standard feature generation approaches, there is no additional information that combinations of these features would find.
However, if we assume that we have access to a relational knowledge base connecting surnames to common countries of origin, we can begin to apply knowledge-based feature generation techniques to the problem, as we can move from the domain of surnames to that of countries. 

 Our goal is to separate the set of people to those at high risk and those at low risk using induction algorithms. We see that the existing feature set is insufficient to perform this task successfully. Therefore, our approach aims to construct new features for the learning task $T_1$ by recursively creating a new learning problem $T_2$. Once we have done so, we can use induction methods on $T_2$ in order to create a classifier that can be used in $T_1$. We construct $T_2$ as follows: 
  The training objects are surnames; surnames of people with the disease are labelled as positive. The features for these new objects are extracted from the knowledge base. In this case, these features are their countries of origin.
Solving the above learning problem through an induction algorithm yields a classifier on surnames that distinguishes between surnames of patients with the disease and surnames of healthy individuals. This classifier for $T_2$ can then be used as a binary feature in the original problem $T_1$ by applying it to the feature value of surname. For example, it can be used as a feature in the node of value female in Figure \ref{fig:tree_base}, yielding the tree seen in Figure \ref{fig:lvl1_tree}. 

This new feature gives us a better generalization over the baseline solution, as we now abstract the long list of surnames to a short list of countries. We can see this as nodes in the classifier for $T_2$ are derived from multiple surnames. This result also allows us to capture previously unseen surnames from those countries. However, this is not a sufficient solution, as we have no way of generalizing on previously unseen countries of origin, and some countries may not have sufficient representation to induce an accurate classifier. %talk about not concept?


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig2.pdf}
	\caption{A constructed feature used within a decision tree}
	\label{fig:lvl1_tree}
\end{figure}

Once again, we require additional knowledge in order to capture the target concept of $T_2$. 
Therefore, we can recursively apply our method while trying to learn $T_2$ in an attempt to better locate the target concept and induce a classifier for $T_1$.
We create a new training set, the objects of which are countries of origin, with countries of surnames belonging to people with high risk are labelled as positive. The knowledge base regarding countries is then used to construct features for this new this training set, giving us a recursive learning problem $T_3$.

We can then use $T_3$ as an input to an induction algorithm. As a result of this process, we end up with a classifier for $T_3$, which tries to separate between countries of origin of people at risk and those not at risk. This classifier will do so by looking at the properties of countries, and reach the conclusion that countries with high average temperature and low precipitation are of high risk: both being characteristic of desert areas.
This classifier is then used on the country of origin feature in $T_2$, yielding a new feature for $T_2$.  This new feature can then be used when inducting a classifier for $T_2$, which, as we have mentioned earlier, is used as a feature for the original problem $T_1$.

The complete process is depicted in Figure \ref{fig:moving_to_lvl2}. The resulting two-level recursive classifier is depicted in Figure \ref{fig:lvl2_tree}. This constructed feature allows us to concisely and accurately capture the target concept, as the construction process is guided by it, in a manner similar to that of deductive approaches.
While this is a simple example, it illustrates how an unsolvable learning problem given the original features can be solved via the use of complex additional knowledge and existing induction methods.

%We also note that while an unsupervised approach can also create a combination of features that when used together allow us to identify the target concept, the unsupervised nature of the search makes such an attempt computationally expensive, especially when the knowledge base is extensive and multiple logical leaps are necessary.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig4_annotated.pdf}
	\caption{Recursive construction of a learning problem on countries of origin. $(1)$-Creating the objects for the new problem. $(2)$-Creating features using the knowledge base. $(3)$-Applying an induction algorithm. $(4)$-The resulting feature.}
	\label{fig:moving_to_lvl2}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig3.pdf}
	\caption{A two-level constructed feature used within a decision tree}
	\label{fig:lvl2_tree}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating Features through Recursive Induction} \label{formal}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We begin our discussion with the standard definition of an induction problem. 
Let $O$ be a set of objects. Let $Y=\{0,1\}$ be a set of labels \footnote{We assume binary labels for ease of discussion}. Let $C:O\rightarrow Y$ be the target concept. Let $S=\{(o_{1},y_{1}),\ldots,(o_{m},y_{m})\}$ be a set of labelled examples such that $o_{i}\in O, y_{i}\in Y, C(o_i)=y_i$. 
Let $F=\{f_{1},\ldots,f_{n}\}$ be a \emph{feature map}, a set of \emph{feature functions} $f_{i}:O\rightarrow I_{i}$.  This definition implies a training set represented by feature vectors: $S_F=\{ (\langle f_1(o_i),\ldots,f_n(o_i)\rangle, y_i) | (o_i,y_i) \in S\}$. A learning algorithm $L$ takes $S_F$ as inputs, and outputs a classifier $h_{S_F}:O\rightarrow Y$.
\begin{defn}
	Let $L(S,F)=h_{S_F}$ be the classifier outputted by $L$ given $\langle S,F\rangle$. Assuming $S\sim\ D$, the generalization error of a learning algorithm $L$ is the probability $Pr(h_{S_F}(x)\neq y)$, where $(x,y)\sim\ D$.
\end{defn}

\begin{defn}
	A \emph{feature generation algorithm} $A$ is an algorithm that given $\langle S,F\rangle$, creates a new feature map $F'=\{f'_{1},\ldots,f'_{l}\}, f'_{k}:O\rightarrow I_k$.
\end{defn}

In order to evaluate the output of a feature generation algorithm $A$, we must define its utility. Given $\langle S,F \rangle$, $A$ generates a feature set $F'_A$.
Given $S\sim\ D$, a feature set $F$, a generated feature set $F'_A$ and a learning algorithm $L$, the utility of $A$ is $U(A(S,F))=Pr(h_{S_F}(x)\neq y)-Pr(h_{S_{F'_A}}(x)\neq y)$, where $(x,y)\sim\ D$.

The implications of this definition are twofold: First, if the classifier induced by $L$ given  $\langle S,F \rangle$ yields a low generalization error, there is little to be gained through feature generation. Second, in order for the utility of $A$ to be positive, the generated feature set $F'_A$ must yield a lower generalization error compared to $F$.

In this work, we assume that in addition to $S_F$ we also have a set of binary \footnote{If our relations are not binary, we can use projection to create multiple binary relations instead} relations ${\cal R}=\{R_{1},\ldots,R_{t}\}, R_j:D_j\times D_{j'}$ representing our knowledge base. 
\begin{defn}
	A \emph{supervised feature generation algorithm} $A$ based on ${\cal R}$ is an algorithm that given $\langle S,F,{\cal R} \rangle$, creates a new feature map $F_{{\cal R}}=\{f'_{1},\ldots,f'_{l}\}, f'_{k}:O\rightarrow I_k$.
\end{defn}

When using a feature generation algorithm based on a set of relations ${\cal R}$, we would like to achieve a higher utility than that achieved without the use of ${\cal R}$. Clearly, in order to do so we must require some connection between ${\cal R}$ and our feature set $F$. To that effect, we add an additional requirement:  $\bigcup_{f_i} I_i \cap \bigcup_{R_j} D_j \neq \emptyset$. Under this assumption, we know that some feature values are also relation keys, allowing us to apply these relations to our learning problem.

\subsection{An unsupervised approach to knowledge-based feature generation} \label{shallow_section}

To begin with, we shall try to ignore the labels, and define an unsupervised feature generation algorithm based on ${\cal R}$. We call this algorithm \emph{Shallow}, as it uses the relations in a shallow manner to create features. Using \emph{Shallow}, we can highlight several important concepts of our approach.

Let us first consider the simplest case, where any given relation $R_j$ is a function $R_j:D_j\rightarrow D_{j'}$. In this case, for any given $f_i,R_j$ such that 
$Image(f_i) \subseteq D_j$, we can compose $R_j$ onto $f_i$, giving us a function $f_{i,j}(x)=R_j\circ f_i=R_j(f_i(x)),f_{i,j}:O\rightarrow D_{j'}$. For example, if $f_i$ lists a person's country of origin, and $R_j$ maps countries to their continent, then $f_{i,j}$ lists a person's continent of origin.
Under this condition, we can define the new feature set generated by Shallow using a closed formula: $\{f_{i,j}|Image(f_i) \subseteq D_j\}$.

Now that we have considered the simple case, we can consider more complex cases.
For one, we must consider the case where $Image(f_i) \not\subset D_j$ but $Image(f_i) \cap D_j \neq\emptyset$. In this case, there are values of $f_i$ for which $R_j$ cannot be applied. We can therefore treat $R_j$ as a partial function from $Image(f_i)$ to $D_{j'}$. In order to complete this partial function, let us first mark \emph{undefined} as $\perp$. We can turn $R_j$ into a full function with regards to $Image(f_i)$ as follows: $\tilde{R}_j(x)=\begin{cases} R_j(x) &\mbox{if } x\in D_j\\ 
\perp & \mbox{otherwise } \end{cases}$.
The result is a full function $\tilde{R}_j:Image(f_i)\cup D_j\rightarrow D_{j'}\cup\{\perp\}$. We also note that the same process can be applied to cases where $R_j$ has missing values\footnote{This is usually the case in human curated knowledge bases.}, meaning it is a partial function with regards to $D_j$. 

Although there are many relations which are also functions, such a requirement may serve as a limitation. For example, a relation mapping a surnames to their countries of origin, as in section \ref{motivation}, may have multiple countries of origin for a single surname. Similarly, a relation mapping a parent to their children may map many children for the same parent.
In cases such as this, composing $R_j$ onto $F_i$ yields a set of values. This set can be used as a set-based feature, but it is usually simpler to utilize an aggregation function on the set of values instead. There are many possible aggregation functions that we can apply to such a value set, but we shall discuss three major ones:
\begin{itemize}
	\item All- Given a set of values, whether they all fulfil a certain predicate $p:D_{j'}\rightarrow\{0,1\}$.
	\item Exists- Given a set of values, whether there exists a value in that set which fulfils a certain predicate $p$.
	\item Majority- Given a set of values, whether a majority of them fulfil a certain predicate $p$.
\end{itemize}
All of the above require a predicate function $p:D_{j'}\rightarrow \{0,1\}$.
As an example, we can check whether a constant $c\in D_{j'}$ is within the values of the set by using $p(x)=\begin{cases}
1 & \mbox{if } x=c\\ 0 & \mbox{otherwise}
\end{cases}$ alongside the ``exists" aggregation method.

Now that we have considered all possible behaviours of a given relation $R_j$, we can proceed to define the \emph{Shallow} algorithm. Given a feature $f_i$, \emph{Shallow} will go over the relations within the knowledge base and attempt to create features $f_{i,j}:O\rightarrow D_{j'}\cup\{\perp\}$ through the discussed techniques. \emph{Shallow} then gathers all generated features and outputs them. The pseudo-code is given below.

\begin{algorithm}[H]
	\caption{\emph{Shallow}: Non-recursive Feature Generation using relations}
	\label{code-compete}
	\small
	%insert param stuff
	\begin{algorithmic}
		\Function{CreateFeatures}{$S$,$F$, ${\cal R}$}
		\State $F_{result}=\emptyset$
		\For {$f_i \in F$}
		\For {$R_j \in {\cal R}$}
		\If {$Image(f_i)\cap D_{j}=\emptyset$}
			\State Continue
		\ElsIf {$R_j:D_j\rightarrow D_{j'}$ and $Image(f_i)\subseteq D_{j}$}
			\State $F_{result}=F_{result}\cup \{f_{i,j}=R_j\circ f_i\}$
		\ElsIf {$R_j:D_j\rightarrow D_{j'}$ and $Image(f_i)\cap D_{j}\neq\emptyset$}
			\State $F_{result}=F_{result}\cup \{f_{i,j}(x)=\begin{cases} R_j( f_i(x)) &\mbox{if } f_i(x)\in D_j\\ 
			\perp & \mbox{otherwise } \end{cases}\}$
		\Else \Comment $R_j$ is a relation $R_j:D_j\times D_{j'}$
		\State $F_{result}=F_{result}\cup \{Aggregate(p,f_{i,j})\}$
		\EndIf
		\EndFor
		\EndFor
		\State \Return $F_{result}$ 
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

We see that this approach allows us to generate features from a relational knowledge base. One possible example of such features is that of semantic type, features making use of the ``IS-A" relation to note the entity type. We note that this approach creates a potentially large number of features, and that a given feature may have a large number of values. While this approach does allow us a degree of generalization, it can only utilize our knowledge base shallowly, from base entities to their values. 
Should we try to apply a chain of relations rather than a single one, we can begin to explore more complex domains. However, we must consider the exponential nature of such a chain. Without any way to guide our search through the knowledge base, we are likely to generate a increasingly large number of relations, many of which will be irrelevant. 

%In some situations, we would like to work with binary features rather than multi-valued ones. In such cases, we can restrict the constructed $f_{i,j}$ by a constant, yielding $f_{i,j,c}(x)=\begin{cases}
%1 & \mbox{if } f_{i,j}(x)=c\\
%0 & \mbox{otherwise}
%\end{cases}$. In the more complex cases, we must consider how to handle missing values and multiple values. To simplify, we can handle $\perp$ as false and use the ``exists" aggregation condition for sets.

\subsection{Creating Learning Problems using Relations} \label{algorithm_section} 

%todo: switch to main flow->branches. make sure the alg shows all the cases!

Now that we have seen how an unsupervised algorithm would make use of a knowledge base for feature generation, we may begin to properly discuss our approach. To do so, we must understand how our approach creates new induction problems using the knowledge base, and how such problems can be used for feature generation.

Given an existing feature $f_{i}:O\rightarrow I_i$, our algorithm will formulate a new learning task trying to separate values in $Image(f_i)$ associated with positive examples of the original learning task from those appearing in negative ones. The result of the new learning task will be a classifier
$h_{i}:I_{i}\rightarrow Y$ which we can then use to label elements in $I_i$, such as those appearing in values of $f_{i}$. We can then define a new feature $f'_{i}(x)=h_{i}(f_{i}(x)), f'_{i}:O\rightarrow Y$.
We name this algorithm \emph{FEAGURE} (FEAture Generation Using REcursive induction). The full pseudo-code is described in algorithm \ref{code-creating-prob}.

In order to explain how we create such $h_{i}$, let us first consider the creation of a new learning problem. Once we have done so, we can use any induction algorithm to create $h_i$. 
Given a feature $f_{i}$, we first define $v_i(S) = \{v | (o,y) \in S, f_{i}(o)=v\}$ the set of feature values for $f_i$ in $S$. In the intro example, for instance, $v_i(S)$ will be the set of all last names of patients that appeared in the training set.
We use $v_i(S)$ as our set of objects. To define a learning problem, we must first label these objects.

In the simple case, every feature value appears only for a single example in $S$. In that case, we can simply take the same label, with the result being $label(v)=y$ where $f_i(x)=v,(x,y)\in S$. In the above example, if each patient has a unique surname, each surname would be labelled according to that patient's label.
Most features in general, and in machine learning tasks in particular, however, do not have a unique value for each example (traditional machine learning algorithms cannot make use of such features). In such cases we would have multiple examples in $S$ such that $f_i(x)=v$. Let us call $S(v)$ the set of such examples. In order for us to give a label for $v$, we must aggregate the labels of objects in $S(v)$ somehow. As in section \ref{shallow_section}, we can consider three major aggregation functions.
Using the All aggregation function results in a strict labelling, as a value will be classified as belonging to the target concept only if all examples in which that value has appeared are labelled positive. The result is the consistent label function: $label(v)=\begin{cases} 1 &\mbox{if } \forall (x,y)\in S(v): y=1\\ 
0 & \mbox{otherwise } \end{cases}$.
 A more lenient label function is the majority label, taking the Majority aggregation function: $label(v)=majority(\{y|(x,y)\in S(v)\})$.
 In the intro example, the consistent label would take the set of patients with the given surname, and label that surname as having high risk only if all such patients have the disease. The majority label, in comparison, would check whether most patients with that name have the disease.
 
We now formulate a new learning problem with the constructed training set
$S'_i = \{ (v, label(v)) | v \in v_i(S) \}$.
To fully define our learning problem, we must specify a feature map over $v_i(S)\subseteq Image(f_i)$. Consider a relation $R_j$ such that $D_j\cap v_i(S)\neq \emptyset$. For such $R_j$, if it is a function, we can simply use it as a feature function%, or restrict by a constant $\left(f_{j,c}(v)=\begin{cases} 1 &\mbox{if } \ R_j(v)=c\\ 
%0 & \mbox{otherwise } \end{cases}\right)$
. If $R_j$ is a partial function, we simply have a feature with missing values. Finally, if $R_j$ is not a function, we can use aggregation as in section \ref{shallow_section} to create features.

Once we have considered all relations in ${\cal R}$, we can generate a feature map for $S'_i$. Given $R_j$, assuming it is relevant to the problem domain, meaning $v_i(S)\cap D_j\neq\emptyset$, we can utilize it as a feature. We then look at all constants $c\in \{R_j(v)|v\in v_i(S)\land R_j(v)\neq \perp\}$. For each such $c$, we add  $f_{j,c}(v)=\begin{cases} 1 &\mbox{if } \ R_j(v)=c\\
\perp &\mbox{if } R_j(v)=\perp\\
0 & \mbox{otherwise } \end{cases}$, $f_{j,c}:I_i\rightarrow \{0,1,\perp\}$ to our feature map. 
For example, in the intro example, each constant $c$ is a country of origin, and $f_{j,c}$ will be 1 if the given surname is from that country of origin, 0 if it isn't and $\perp$ if the surname does not appear in our knowledge base.
The result of this process is a feature map for $S'_i$ that includes all such $f_{j,c}$, denoted as $F_{\cal R}$. We now have a complete induction problem $\langle S'_i,F_{\cal R} \rangle$ for which we can train a classifier, giving us $h_i:I_i\rightarrow Y$. We can then use this $h_i$ on objects in $S$ as discussed above, giving us $f'_{i}(x)=h_{i}(f_{i}(x)), f'_{i}:O\rightarrow Y$. 

We note that once we have an induction problem over $I_i$, we can apply our approach recursively on $S'_i$ in order to create additional features. We can see an example of this in section \ref{motivation}, wherein we construct a second-level problem on countries of origin to correctly identify the appropriate conditions that signify high risk countries.
 For each relation $R_j$, we take its values, and treat those as a feature $f_j(v)=\{v'|R_j(v)=v' \}$. We can then proceed to construct a learning problem over values $f_j$ in the same manner as before, yielding a classifier $h_j:D_{j'}\rightarrow Y$, which can then be used to create a feature for $S'_i$. 
 
 In cases where $R_j$ is a relation rather than a function, $f_j(v)$ is a set of values- a relation. In such cases, We must discuss both how to extract a new learning problem, and how a classifier is then used for a set of features.
 To begin with, let us tackle the issue of constructing a learning problem when each feature value is a set. In such cases, we still have the above problem where multiple examples may contain the same value, but a single example has multiple distinct values. One possible solution to this issue is to give each value of a given example the same label as that of the example, and proceeding to resolve multiple labels via aggregation as we have discussed previously. 
 Once we have done so, we can construct a new learning problem for values of $f_j$ and, using FEAGURE, receive a classifier $h_j:D_{j'}\rightarrow Y$.
 Another question we must ask ourselves is how such a classifier can be applied to the set of values in $f_j(v)$. To illustrate, consider the classifier shown in Figure \ref{figure5}. We see that the classifier can be applied to multiple values of the feature, giving us a set of labels as the label of our example. As we have already discussed how such a set of labels should be resolved when discussing the labelling of the constructed learning problem, we can apply aggregation functions to this issue, giving us a single label for our example in $S'_i$.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.8]{fig6.pdf}
	\caption{Using a constructed classifier on a set of values: $(1)$-Feature values are used to construct a learning problem. $(2)$-The classifier is then applied to each value. $(3)$-An aggregation function is applied on the results. In this case, the majority function is used to determine the output for the learning problem.}
	\label{figure5}
\end{figure}

In many feature generation approaches, feature selection methods play a key part in filtering out poor features. In FEAGURE, we can use feature selection both as a filtering mechanism, as well as a search parameter.
As a filtering technique, the features generated by FEAGURE are compared to the already existing feature map $F$. There is a multitude of feature selection techniques that can be applied for this task, but we shall cover three filtering criteria.
\begin{enumerate}
	\item Maximal Information Gain (IG)- The information gain metric \cite{quinlan1986} compares how well a single feature reduces the amount of information entropy within a given labelled set should we split by it. In essence, it measures how well each feature separates the set based on its labels. Taking the maximal information gain as a metric, we measure whether the constructed feature gives a better separation compared to all existing features in $F$.
	\item Average Information Gain- Just as we can compare our feature to the maximal IG of the feature map, we can compare it to the mean IG for features in $F$.
	\item Sufficient Diversity- IG based approaches measure the separation of the training set. In many feature generation tasks, however, the more correct approach is to generate features which differ from existing features. To do so, we could require that the correlation of the generated feature with features in $F$ be below a certain threshold.
\end{enumerate}
Beyond their use for filtering, criteria that measure the performance of features such as the ones above can be used as a deciding factor in whether or not a constructed learning problem should apply FEAGURE recursively. The idea here is that should the generated feature be close to the threshold, we may wish to continue expanding it in hopes of giving that feature the necessary boost in terms of the specific criteria.

\begin{algorithm}[H]
	\caption{FEAGURE-FEAture Generation Using REcursive induction}
	\label{code-creating-prob}
	\small
		%insert param stuff
		\begin{algorithmic}
			\Function{ConstructFeatures}{$F$, $S$, ${\cal R}$}
				\For {$f_i\in F$}
				\State $S'_i,F_{\cal R}$= \Call{CreateNewProblem}{$f_i$,$S$,${\cal R}$} \Comment{We can apply our algorithm recursively here}
				\State $h_i$= \Call{InductionAlgorithm}{$S'_i,F_{\cal R}$} 
				\If {\Call{Compare}{$h_i,F$}} \Comment Compare $h_i$ to $F$
				\State add $h_i$ to new features
				\EndIf
				\EndFor
				\State \Return new features
			\EndFunction
			\State 
			\Function{CreateNewProblem}{$f_{i}$, $S$, ${\cal R}$}
                \State $v_i(S) = \{v | (o,y) \in S, f_{i}(o)=v\}$
                \State $S'_i = \{ (v, label(v)) | v \in v_i(S) \}$ 
                \Comment $label(v)$ can be, for example, the majority function.
                \State Let $f_{j,c}(v)=\begin{cases} 1 &\mbox{if } \ R_j(v)=c\lor c\in R_j(v) \\ 
                \perp & \mbox{if} R_j(v)=\perp\\
                0 & \mbox{otherwise } \end{cases}$
                \State $F_{\cal R}=\{f_{j,c}| c\in D_{j'}, R_j\in{\cal R}\}$
                \State \Comment We only need $c$ such that $\exists v\in v_i(S):f_{j,c}(v)=1$
                \State \Return $S'_i, F_{\cal R}$ 
			\EndFunction
			
		\end{algorithmic}
	\end{algorithm}

We note that this feature generation approach creates features which can be used alongside any induction algorithm. Additionally, any induction algorithm can be used to learn $h_i$. Due to this generality, the extensive knowledge and literature available for induction methods applies in full. Furthermore, the recursive nature of FEAGURE allows for a structured search of the knowledge base, with each new recursive problem modelling a projection of the problem space to the new domain, allowing for a process similar to deductive learning. For example, in the motivating example, we began by learning a problem regarding patients, which we projected to a new domain of surnames. We then re-contextualized that problem to the domain of countries, for which we located a good solution. We then used this solution to resolve the original learning task concisely and accurately.

While this approach has many benefits, there are is a potential issue with this approach, namely, that the number of features generated by it is limited by the number of features in the original problem. There are several methods to resolve this issue:
\begin{enumerate}
	\item Using multiple induction algorithms on the generated learning problem may yield significantly different classifiers which can then be used as unique features, in a manner resembling an ensemble approach.
	\item Use of feature selection methods within the generated problem allows for multiple classifiers to be learned by reusing the same feature map. This approach has the potential downside that later features may be significantly worse.
	\item Some induction algorithms, the chief of which being the class of decision tree classifier methods, yield decomposable classifiers. In the case of decision trees, such a tree classifier can be flattened into a set of decision stumps, each holding a single split of the tree. This approach allows us to turn a single strong feature into multiple weaker ones.
	\item By sub-sampling our training set, we can learn features that only apply to a part of the training set, thus potentially giving us different, local features.
\end{enumerate}

\subsection{Finding Locally Improving Features} \label{tree_usage}

As we have previously discussed, in some cases creating multiple features is superior to generating only a few. Beyond this, in many cases it is far easier to locate powerful features within a localized subset of data which shares common attributes rather than trying to locate the same features in a more global context. In the motivating example, for instance, male patients are irrelevant to the target concept, and thus serve as noise when used for feature generation.
There is a multitude of approaches to creating such local features. We shall consider a few of these approaches:
\begin{itemize}
	\item Sub-Sampling: By sampling a subset of training examples, we can hope to create a good local context for feature generation. An issue with this approach is that there is a huge number of possible subsets, and thus going over all of them is not computationally feasible. 
	\item Clustering: This approach attempts to make use of the feature set to create clusters of similar examples. In some cases, this would indeed yield good local contexts, but this approach is potentially problematic for our algorithm, as it is very likely that similar examples will have the same label, thus creating highly unbalanced learning problems, which would be more difficult to utilize effectively.
	\item Divide \& Conquer: This approach seeks to iteratively create increasingly smaller subsets of the original problem by dividing the problem according to the values of a feature. Most often, the IG metric is used to determine the feature used to split the examples.
\end{itemize}

Of the above approaches, the Divide \& Conquer approach yields numerous advantages we can make use of, namely:
\begin{enumerate}
	\item Orthogonality: Because all examples with the same value for a given feature are grouped together, any further splits must make use of different features. Due to this and the fact that the features selected in each step have high IG, it is usually the case that features chosen later in the process will be mostly orthogonal to previously chosen features. This results in a larger variety of features overall. In our approach in particular, the elimination of a feature prunes the search tree and forces later splits to rely on different features and thus different domains.
	\item Interpertability: Looking at the features used at each splitting point gives us an intuitive understanding of the resulting subsets. Because of this, we can more easily understand why certain features were picked over others, which domains are no longer relevant, and so on.
	\item Iterative construction: The divide \& conquer approach allows for an iterative search process, which we can then easily interrupt if a sufficient number of features were generated, or when the remaining training set is no longer sufficiently representative for drawing meaningful conclusions.
	\item Guided by labelling: This approach explicitly uses the labels of the training set to create meaningfully different groups. As a result of this, we can expect that as we go further down, the need for good, distinguishing features will rise, and we can locate stronger features.
\end{enumerate}

Due to the above advantages, we have a strong incentive to utilize the divide \& conquer approach when attempting to create additional features using FEAGURE. This new algorithm, which we shall name \emph{Deep-FEAGURE}, begins by taking the entire training set and applying FEAGURE to it. Once features have been generated, any generated features which have performed well are kept, and the feature with the highest information gain (potentially the one generated by FEAGURE) is used to split the training set into groups according to its values.
We then repeat this process for each of these new, smaller training sets, until we either receive a consistent training set, meaning all examples have the same label, or the size of the training set becomes so small that it is wholly unrepresentative of the complete problem.
Once we have finished this process, only the generated features are kept, and returned as the output of Deep-FEAGURE.

A major potential issue that must be considered in any induction approach, and is especially problematic for divide \& conquer approaches, is that of over-fitting, meaning that the result is too adjusted to seen data, and cannot generalize to previously unseen data. In the extreme case, an induction method may yield a classifier that simply memorizes the known data. This classifier will yield perfect results for seen data, but is worthless for unseen data, and will result in a high generalization error.
To combat this issue, we consider several steps:
\begin{itemize}
	\item Limiting the minimal size of the training set allows us to control for unrepresentative subsets, which is a major factor in over-fitting.
	\item Feature selection inherent to FEAGURE limits the potential for over-fitting, as generated features must be beneficial to enter the feature pool. It is important to mention, however, that a very strict feature selection criterion may have an opposite effect, as any feature that pass it must be very well fitted to the training data.
	\item Limiting the depth of search by refusing to further split beyond a certain depth is another way to prevent unrepresentative subsets. Of particular notice, the number of relations in our knowledge base gives us a natural depth limit due to the orthogonal nature of this approach. Essentially, once a certain relation has been used, it is unlikely that another feature down the tree will make use of it again, as we have already split the training set according to values of that domain. 
	\item Well-known induction approaches often acknowledge the issue of over-fitting, and offer multiple techniques to minimize it. Should we use search trees, for example, flattening can be used to create a set of weaker features, which is less likely to over-fit than a single complex decision tree.
\end{itemize}

\begin{algorithm}[H]
	\caption{Deep FEAGURE- Divide \& Conquer Feature Generation}
	\label{code-tree-thing}
	\small
		minSize- minimal size of a node.

        SplitByBestFeature- a method that splits a training set according to a feature.
        
		\begin{algorithmic}
			\Function{ConstructFeaturesInANode}{$S$, $F$, ${\cal R}$}
                \If {$|S|<$minSize} 
                    \State
                    \Return 
                \EndIf
                \State newFeatures=\Call{FEAGURE}{$S$, $F$, ${\cal R}$}
                \State \Return \Call{SplitByBestFeature}{$S$, $F\cup$ newFeatures}, newFeatures
			\EndFunction

            			
			\State 
            \Function{ConstructFeatures}{$S$, $F$, ${\cal R}$}
                \State currentFeatures= $\emptyset$
                \State sons, $F_{new}$=\Call{ConstructFeaturesInANode}{$S$, $F$, ${\cal R}$}
                \State currentFeatures= currentFeatures$\cup F_{new}$
                \For {son in sons}
                    \State newFeatures=\Call{ConstructFeatures}{son.$S$,$F$,${\cal R}$}
                    \State currentFeatures= currentFeatures$\cup$newFeatures
                \EndFor
                \State \Return currentFeatures
			\EndFunction
		\end{algorithmic}
	\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we discuss our experimental methodology, detail the algorithms, datasets and knowledge bases we utilize and display our main results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Application of FEAGURE for Text Categorization} \label{text-feagure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A particually interesting problem domain is that of \emph{Text Categorization}.
The text categorization problem is defined by a set of texts $O$ labelled by a set of categories $Y$ \footnote{We can assume $Y=\{0,1\}$ for ease of analysis}
such that we create $S=\{(o_i,y_i)|o_i\in O, y_i\in Y\}$. Given $S$, The learning problem is to find a hypothesis $h:O\rightarrow Y$ which minimizes generalization error over all possible texts of the given categories. To measure this error, a testing set is used as an approximation.
The traditional approach to solving this problem is based on the bag-of-words \cite{Wu:1981:CST:1013228.511759,salton1983introduction} approach, which uses the frequencies of word appearances (without regards to order) within the text as features. 

Another way to describe this approach is that it creates a single feature, the titular bag-of-words, whose value is the set of all unique words in the text.
As we are well aware, however, not all words have semantic meaning. This observation makes usage of knowledge-based approaches non-trivial, as most knowledge bases rely on semantically meaningful entities rather than raw words.
To resolve this issue, we can make use of techniques such as Named Entity Recognition (NER), Wikification \cite{bunescu2006using} and Entity Linking \cite{rao2013entity}. These methods as well as similar techniques allow us to link words in the text with semantically meaningful entities. They do so by identifying keywords, linking related concepts, and other supervised and unsupervised techniques.
Using these techniques, we can move to a \emph{bag-of-entities} representation, that is, a model of the semantically meaningful entities within the text, without regard to order of appearance.

As discussed, we have seen the recent rise of Semantic Linked Data as a powerful knowledge base for text-based entities, with large databases such as Google Knowledge Graph \cite{pelikanova2014google}, Wikidata \cite{vrandevcic2014wikidata} and YAGO2 \cite{hoffart2013yago2} becoming common. 
These knowledge bases represent semantic knowledge through the use of relations, mostly represented by triplets of various schema such as RDF, or in structures such as OWL and XML. These structures conform to relationships between entities such as "born in", as well as type information.
Naturally, we would like to make use of such semantic data to improve upon the bag-of-words and bag-of-entities approaches, with the implicit assumption that additional Semantic knowledge will allow us to better approximate the relationship between the text and its given category.

A major topic to consider when trying to apply FEAGURE to this domain is the fact that the bag-of-entities approach inherently creates features with many values. Because of this, special care must be taken to both the construction of recursive problems (a single example may have multiple applicable values) and the application of the resulting classifier on the document (several entities may apply).
Naturally, many entities within a given document will not apply to a given relation as the domains differ, and thus yield a value of $\perp$. To combat this, we only give a document the value of $\perp$ if it contains no entities which are relevant to the classifier.

\subsection{Algorithms}

In order to test the effectiveness of FEAGURE, we compared three feature generation algorithms:
\begin{enumerate}
	\item Shallow: This unsupervised algorithm, defined in section \ref{shallow_section}, allows us to see the effects of the knowledge base on accuracy in general. Through this algorithm, we can better measure the effectiveness of FEAGURE.
	\item FEAGURE: We applied FEAGURE to the learning problem, and constructed recursive learning problems. Each such new problem was then solved and used as a feature.
	\item FEAGURE 2-level: This algorithm is the 2-level application of FEAGURE, as demonstrated in section \ref{motivation}. During the activation of FEAGURE, we applied the algorithm recursively on each generated learning problem, allowing us to locate deeper relationships within the dataset.
\end{enumerate}

\subsection{Methodology}

We have evaluated our algorithm in the domain of text categorization problems.
As we have discussed in section \ref{formal}, in order to evaluate a feature generation algorithm, we must first define an induction problem in order to measure generalization error and utility. Once we have done so, we activate the algorithm on the training set in order to generate new features. Finally, once we have generated our new feature set, we proceed to test it by learning a classifier on the training set and measuring its accuracy against a testing set \footnote{This testing set is treated as a sample of the object space $O$}, compared to the baseline approach without the use of feature generation. We made use of three well-known learning algorithms for this task: SVM \cite{cortes1995support}, K-NN \cite{fix1951discriminatory} and CART \cite{breiman1984classification}\footnote{For SVM we used a linear kernel and a regularization parameter $C=10$; for K-NN we used $K=3$; for CART we used a minimal leaf size proportional to training set size}.
The use of multiple induction algorithms allows us to reduce the impact of the specific induction approach in evaluating the performance of the generated features.


\subsubsection{Datasets and Knowledge Bases}

In order to best evaluate our approach, we wished to test it on a wide array of text classification datasets. To that end, we focused on the TechTC-100 \cite{gabrilovich2004text} dataset collection.

\textbf{TechTC-100} is a collection of 100 different binary text categorization problems of varying difficulty, extracted from the Open Dictionary project.
On each dataset, we evaluated our approach  by measuring accuracy based on the training and testing sets defined in the original paper. 

As our knowledge base for this task, we used YAGO2 \cite{hoffart2013yago2}.
YAGO2 (Yet Another Great Ontology) is a large general knowledge base extracted automatically from Wikipedia, WordNet and GeoNames.
YAGO2 contains over 10 million entities and 124 million facts, mostly dealing in individuals, countries and events.
In order to fully make use of this knowledge base, we performed some processing on it. Specifically, we omitted relations with literal data such as dates or geographic coordinates, and created inverse relations of all relations except ones that map very few values to very many \footnote{For example, the inverse of the ``has gender" relation  maps ``male" and ``female" to all to all existing people within the YAGO2 database.}.

For entity extraction, we used AIDA \shortcite{hoffart2011robust}, a framework and online tool for entity detection and disambiguation. AIDA uses the Stanford NER tagger \cite{finkel2005incorporatingfull} to locate entity candidates, and attempts to link them to existing entities within the YAGO2 knowledge base.

The TechTC-100 dataset collection served as a powerful benchmark, as it shows results for multiple problems at once, giving a broad view for many problems with varying difficulties. This aspect allowed us to better understand and analyse our approach in a variety of situations.

Following our experiments on TechTC-100, we also wanted to explore the results of our approach with regards to a more domain-specific problem.
To that end, we constructed a dataset based on OHSUMED \shortcite{hersh1994ohsumed}.

\textbf{OHSUMED} is a large dataset of medical abstracts from the MeSH categories of the year 1991. 
We constructed our own dataset based on the work of \citeA{joachims1998text}.
First, we took the first 20,000 documents, similarly to \citeA{joachims1998text}.
Then, we limited the texts further to medical documents that contain only a title (that is, there is no available abstract). On each document we used stopword elimination and a simple stemming scheme (due to the medical nature of the texts, there was no need for complex stemming) in order to remove unnecessary noise. Once the documents have been cleaned of stopwords and stemmed, we used Wikipedia Miner \cite{milne2013open} for entity extraction. 
Due to the relatively sparse size of most MeSH categories (since many documents were classified using the abstract), we only used the two categories with the most documents, ``Bacterial Infections and Mycoses" and ``Immunologic Diseases" \footnote{C1 and C20, respectively}, yielding a binary learning problem.
The result is a dataset of 850 documents of each category, for a total of 1700 documents.

Ten-fold cross validation was used in order to evaluate the accuracy of algorithms on the dataset and measure statistical significance.

As the YAGO2 knowledge base does not contain medical facts, we sought to use a more appropriate knowledge base for our task. To that end, we opted to use \textbf{Freebase}.
Freebase has been described as ``a massive, collaboratively edited database of cross-linked data". Freebase is constructed as a combination of data harvested from databases such as Wikipedia and data contributed by users. The result is a massive, extensive knowledge base containing 1.9 billion facts. 
We used the same freebase data dump used by \shortciteA{bast2014easy}, taking only entities relevant to our domain. This smaller dump of the Freebase knowledge base contains approximately 49 million entities and 242 million facts spanning multiple domains. Data regarding the medical domain is far more sparse, containing roughly a hundred thousand facts in total.

Due to it's inclusion of domain specific knowledge, Freebase is more suitable for problems specializing in a specific subject, such as the OHSUMED dataset. 
This dataset served as a test case for a more specialized learning problem, where domain-specific knowledge is required.

In general, we do not expect a single knowledge base that will apply to any and all problem domains, but rather that each problem domain make use of a more tailored, domain-appropriate knowledge base. By doing so, we can hope to better solve these problems.

\subsubsection{Experiment Parameters}

We tested the following parameters: 
\begin{enumerate}
	\item Recursion level: We ran our feature generation algorithm for both a depth of one, creating a recursive learning problem for the original problem, and a depth of two, creating recursive learning problems within generated problems. This parameter allowed us to test the effect of both first and second order application of our feature generation approach.
	%\item Tree depth: As discussed in section \ref{tree_usage}, we use Deep-FEAGURE within a decision tree. In order to avoid over-fitting, we limit both tree depth and node size, proportionately to the size of the training set. We experimented with the parameter of maximal tree depth to test its effect on the accuracy of the resulting features.
	%\item FEAGURE internal induction algorithm: In section \ref{algorithm_section}, we define FEAGURE without specifying the internal induction algorithm used to generate features within the new learning problem. We ran most of our experiments using a decision tree induction algorithm for that purpose, flattening the result to yield additional features. We also experimented with the use of other induction algorithms, namely SVM and K-NN.
	%We note that this choice is orthogonal to that of the induction algorithm used to evaluate the generated features.
	%\item Feature filtering criterion: In section \ref{algorithm_section}, we use a comparison function to decide whether a generated feature should be added to the feature map. We experimented with both a comparison based on maximal Information Gain (IG) and one based on average IG, both taking into account all existing features in $F$.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{table:acc} shows average accuracies across all 10 folds for OHSUMED, as well as the average accuracies for all 100 datasets in techTC-100. Statistical significance over the baseline accuracy (without feature generation) is shown in parenthesis. Best results are marked in bold.
For the TechTC-100 dataset, we see a significant improvement over the baseline and shallow approaches, even though the number of generated features is much lower than the one achieved by the shallow algorithm, as seen in table \ref{table:features}.
Of particular note are the results for KNN and SVM, where the two level activation of FEAGURE shows statistically significant improvement over Shallow, and not just the baseline. 

Analysis of the results for the OHSUMED datasets show that for K-NN, the shallow algorithm performs more poorly than the baseline approach, meaning the features generated by shallow were not beneficial to the induction algorithm. For FEAGURE, we also see a degrade in accuracy, due to the masking effect inherent to K-NN classifiers, meaning that impact of a single distinguishing feature may be masked by other features, and addition of features tends to degrade performance. For SVM, we see a significant improvement over the baseline approach, with a single level application achieving an average of $2.5\%$ increase in accuracy, and a two-level application giving a total average of $2.75\%$ improvement. For CART, we also see a general trend of improvement, with results for a two level application of FEAGURE showing significant improvement.

%\begin{itemize}
%	\item Errors in the entity extraction process may lead to the creation of misleading entities and thus features. For instance, the word ``One" may be interpreted as the entity ``One (Metallica song)". 
%	\item Over-fitting within the feature generation algorithm may create features that appear to have high information gain while generalizing poorly to test data.
%	\item Small dataset size may lead to small, unrepresentative learning problems.
%\end{itemize}

\begin{table}[]
	\centering
	\caption{Average accuracy over all datasets. The columns specify feature generation approach, with baseline being no feature generation. The rows specify the induction algorithm used on the generated features for evaluation.}
	\label{table:acc}
	\begin{tabular}{|l | l || l | l | l| l|}
		\hline
		Dataset & Classifier & Baseline   & Shallow & FEAGURE   & FEAGURE 2-level    \\ \hline
		\multirow{3}{*}{OHSUMED} & KNN  & \textbf{0.777} & 0.756 & 0.769   & 0.75 \\ \cline{2-6}
		& SVM  & 0.797 & 0.804   & 0.816 ($p<0.05$)    & \textbf{0.819 ($p<0.05$)} \\ \cline{2-6}
		
		& CART  & 0.806 & 0.814   & 0.809    & \textbf{0.829 ($p<0.05$)} \\
		
		\specialrule{.15em}{.05em}{.01em} % \hline
		
		\multirow{3}{*}{TechTC-100} & KNN & 0.531 & 0.702 ($p<0.001$) & 0.772 ($p<0.001$) & \textbf{0.775 ($p<0.001$)}  \\ \cline{2-6}
		& SVM  & 0.739 & 0.782 ($p<0.001$)    & 0.796 ($p<0.001$)    & \textbf{0.807 ($p<0.001$)} \\ \cline{2-6}
		
		& CART  & 0.81 & 0.815   & 0.814   & \textbf{0.825 ($p<0.05$)}  \\
		
		\specialrule{.15em}{.05em}{.01em}
		 
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Average number of generated features for each dataset per fold/dataset, using FEAGURE, a 2-level activation of FEAGURE, and the Shallow algorithm. }
	\label{table:features}
	\begin{tabular}{|l||l|l|l|}
		\hline
		& \# Features(FEAGURE)  & \# Features(FEAGURE 2-level)  & \# Features(Shallow) \\ \hline
		OHSUMED      & 506           & 732        & 27162               \\ \hline
		TechTC-100  & 614       & 1477      & 10997 \\ 
		\hline             
	\end{tabular}
\end{table}

%results for svm, results for knn, discuss differences and compete-alg.
Before we look at varying parameters of our algorithm, let us analyze the results on TechTC-100 to better understand the impact of our approach on datasets of varying difficulty. Figures \ref{fig:svm_base_lvl1} and \ref{fig:svm_base_lvl2} show the accuracies for datasets in techTC-100 using a SVM classifier. The x axis represents the baseline accuracy without feature generation, and the y axis represents the accuracy using our new feature set using FEAGURE. Therefore, any dataset that falls above the $y=x$ line marks an improvement in accuracy. We use this visualization technique to better illustrate our results on the techTC-100 dataset collection.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{svm_full}
	\caption{Accuracy of
		baseline approach compared to single activation of FEAGURE (SVM). Each point represents a dataset. The dotted lines represent a 5 and 10 percent difference in accuracy}
	\label{fig:svm_base_lvl1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{svm_full_lvl2}
	\caption{Accuracy of
		baseline approach compared to two-level activation of FEAGURE (SVM). Each point represents a dataset. The dotted lines represent a 5 and 10 percent difference in accuracy}
	\label{fig:svm_base_lvl2}
\end{figure}

The results show a strong trend of improvement, with high ($> 5\%$) or very high ($>10\%$) improvement being common.
For a single activation, we see 15 datasets where no change in accuracy is observed. This is due to a low number of generated features, as very few entities have been extracted. For 13 datasets, we see a degrade in accuracy, 5 of which showing  a degrade of over $> 5\%$.
For a two-level activation, 12 datasets show no change in accuracy, and only 8 datasets show a degrade in accuracy, with the same 5 datasets showing notable degrade.

This degrade in accuracy can be explained due to a combination of several factors:
\begin{itemize}
	\item Errors in the entity extraction process cause potential entity candidates to remain hidden, thus denying their use in later stages.
	\item Errors in the entity linking process may lead to the creation of misleading entities and thus features. For instance, the word ``One" may be interpreted as the entity ``One (Metallica song)".
	\item Over-fitting within the feature generation algorithm may create features that appear to have high information gain while generalizing poorly to test data.
\end{itemize} 

In their paper on TechTC-100, \citeA{gabrilovich2004text} define a metric known as Maximal Achievable Accuracy (MAA). This criterion attempts to define the difficulty of the induction problem by finding the maximal accuracy among three induction algorithms (SVM, K-NN and CART \footnote{\citeA{breiman1984classification}}).
Intuitively, a dataset with low MAA can be considered harder than one with a high MAA, since a known induction algorithm can achieve a higher accuracy for it.
Figure \ref{fig:25best} shows the 25 hardest datasets in TechTC-100, in terms of the MAA criterion. 
We see that for 5 of these, there is no change in accuracy, either due to the lack of features or due to lack of contribution in the final classifier.
For 2 of the 25 datasets, we see a minor improvement in accuracy. For the remaining 17 of them, we see an improvement of $5\%$ or above, with the highest being over $20\%$.
Of the 25 datasets, only a single dataset shows a degrade in accuracy. These results illustrates that we can, in general, rely on FEAGURE to yield positive features for difficult classification problems.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{25best}
	\caption{Accuracy of
		baseline approach compared to single-level activation of FEAGURE (SVM). Displayed are the 25 hardest datasets (meaning they have the lowest MAA)}
	\label{fig:25best}
\end{figure}

%\subsection{Using Non-Tree classifiers with FEAGURE}

%As we have discussed in section \ref{algorithm_section}, algorithm \ref{code-creating-prob} creates a generic learning problem as part of its execution. In this section, we will test the effects of using various induction algorithms on these recursive problems. We have chosen to try both K-NN and SVM, with the following parameters: For K-NN, we used $K=3$. For SVM we used $C=10$, with both Linear and a Radial Basis Function (RBF) kernels.

%Table \ref{table:features-nontree} shows the average number of generated features. We see that these approaches generate far fewer features on average. This should come as no surprise, as we cannot flatten constructed recursive trees in order to generate additional features, nor do we run the learning algorithms on smaller problems.
%Table \ref{table:acc-nontree} shows the accuracies achieved by using these induction algorithms. We see that while in general, the results are slightly poorer than those achieved by our approach, they are roughly comparable, and in some cases perform noticeably better. One particular such case is that of K-NN for the OHSUMED dataset, which was previously the only case where no improvement in accuracy was achieved. Using an SVM classifier to generate features rather than tree-based features, we see an improvement in accuracy for this case. This may be due to the nature of RBF-based classifiers, which look at distance between examples, similarly to K-NN.
%Looking at the number of features generated, we see that on average, these approaches generate far fewer features. This should come as no surprise, as the tree-based classifier uses flattening to create additional features.

%\begin{table}[]
%	\centering
%	\caption{Average accuracy over all datasets. The columns specify the induction algorithm used in FEAGURE (Decision Tree, SVM with linear or RBF kernel, K-NN). The rows specify the induction algorithm used on the generated features for evaluation. TechTC-100 has an additional entry for 5\% feature selection, meaning only 5\% of all features are used.}
%	\label{table:acc-nontree}
%	\centering
%	\begin{tabular}{|l | l || l | l| l|l|}
%		\hline
%		Dataset & Classifier  & Tree-FEAGURE & Linear SVM   & RBF SVM & 3-NN    \\ \hline
%		\multirow{2}{*}{OHSUMED} & KNN  & 0.761 & 0.756   & \textbf{0.783} & 0.744 \\ \cline{2-6}
%		& SVM   & \textbf{0.814}  & 0.795  & 0.796 & 0.788 \\ \specialrule{.15em}{.05em}{.01em} % \hline
		
%		\multirow{2}{*}{TechTC-100} & KNN  & \textbf{0.533} & 0.531 ($p<0.05$) & 0.527 &  0.528 \\ \cline{2-6}
%		& SVM    & \textbf{0.712} &  0.705 ($p<0.005$)  & 0.698 ($p<0.05$) & 0.697 \\ \specialrule{.15em}{.05em}{.01em}
		
%		\multirow{2}{*}{TechTC-100 (5\%)} & KNN  & \textbf{0.629 ($p<0.001$)}  & 0.625 ($p<0.05$) & 0.61 & 0.617 \\ \cline{2-6}
		
%		& SVM   & \textbf{0.707 ($p<.001$)} & 0.697 & 0.7 ($p<0.05$) & 0.699 ($p<0.05$)\\ \hline
		
%	\end{tabular}
%\end{table}

%\subsection{The Effects of Feature Filtering on Accuracy}
%\label{result-maximal-average}

%In this section we look at the effect of using a different evaluation method for generated features. In algorithm \ref{code-tree-thing}, we use a comparison function which compares the constructed feature to the existing feature map. 
%For the most part, we compared the IG Ratio of the constructed feature to the maximal IG Ratio of features in $F$ to do so.
%An alternative method to this is to use the mean IG Ratio of features in $F$. Should our new feature score lower than the average, we filter it out. 

%\begin{table}[]
%	\centering
%	\caption{Average Number of Generated Features}
%	\label{table:features-average}
%	\begin{tabular}{lllll}
%		& FEAGURE  & FEAGURE 2-level  & Average IG &  Average IG 2-level \\
%		OHSUMED      & 506.2           & 732        &    1270.6   &   1371.4       \\
%		TechTC-100  & 63.3       & 65.06      &  234.43 &  244.05             
%	\end{tabular}
%\end{table}

%\begin{table}[]
%	\centering
%	\caption{Average accuracy across all datasets. The columns specify feature filtering approach, being either the average information gain (IG) or the maximal. The rows specify the induction algorithm used on the generated features for evaluation. TechTC-100 has an additional entry for 5\% feature selection, meaning only 5\% of all features are used.}
%	\label{table:acc-average}
%	\begin{tabular}{|l | l || l | l || l| l|}
%		\hline
%		Dataset & Classifier & Max-IG   & Average-IG & Max-IG 2-level  & Average-IG 2-level    \\ \hline
%		\multirow{2}{*}{OHSUMED} & KNN  & \textbf{0.761} & 0.667 & 0.744   & 0.686 \\ \cline{2-6}
%		& SVM  & \textbf{0.814} & 0.811   & 0.808    & 0.81 \\ \specialrule{.15em}{.05em}{.01em} % \hline
		
%		\multirow{2}{*}{TechTC-100} & KNN & 0.533 & 0.553 ($p<0.001$) & 0.552 & \textbf{0.555 ($p<.001$)}  \\ \cline{2-6}
%		& SVM  & 0.712 & \textbf{0.728 ($p<0.001$)}    & 0.691   & 0.658 \\ \specialrule{.15em}{.05em}{.01em}
		
%		\multirow{2}{*}{TechTC-100 (5\%)} & KNN  & 0.629 & \textbf{0.646 ($p<0.001$)} & 0.619   & 0.594 \\ \cline{2-6}
		
%		& SVM  & 0.707 & \textbf{0.724 ($p<.001$)}   &0.704 & 0.707 ($p<0.05$) \\ \hline
		
%	\end{tabular}
%\end{table}

%Table \ref{table:features-average} shows the amount of features generated by FEAGURE. We see that the amount is significantly greater, indicating that this is a more lax requirement than the one we used (over half of the best information gain ratio). 
%Table \ref{table:acc-average} shows the accuracies attained through this change. For the TechTC-100 dataset collection, we see a noticeable increase in accuracy, especially for single level application. For the OHSUMED dataset, however, we cannot detect a meaningful improvement, and for K-NN see a significant degrade in accuracy. 
%Looking at he number of features generated reveals that using the average IG method for filtering yields roughly three times as many features for TechTC-100 (compared to using maximal IG ratio). For the OHSUMED dataset, the number of features generated is roughly twice as much as that seen when using maximal IG, and that number is already quite high. 
%These results suggest that for small problems, generating additional features may be preferable, whereas for ones with many examples, a stronger filtering mechanism is more advantageous. 


%\subsection{The Effect of Search Tree Depth on Accuracy}

%In this section, we test at the usefulness of searching local problems for additional features.
%To do so, we ran FEAGURE while limiting the depth of the search tree in algorithm \ref{code-tree-thing}. 
%We compare our two datasets side-by-side.

%\begin{figure}
%	\centering
%	\begin{subfigure}{.55\textwidth}
%		\centering
%		\includegraphics[width=1.1\linewidth]{num_features_ohsumed.pdf}
%		\caption{OHSUMED}
%		\label{fig:features-depth-ohsumed}
%	\end{subfigure}%
%	\begin{subfigure}{.55\textwidth}
%		\centering
%		\includegraphics[width=1.1\linewidth]{num_features_techtc.pdf}
%		\caption{TechTC-100}
%		\label{fig:features-depth-techtc}
%	\end{subfigure}
%	\caption{Mean number of features generated}
%	\label{fig:features-depth}
%\end{figure}

%Figure \ref{fig:features-depth} shows the number of generated features per maximal depth.  For both datasets, we see a linear increase, with a two-level application showing a faster increase. In TechTC-100, we see that after a certain depth, there is no additional gain. Due to the small size of the learning problems, beyond that depth the remaining training sets are too small to continue feature generation without significant risk of over-fitting, and thus FEAGURE stops its execution.

%\begin{figure}
%	\centering
%	\begin{subfigure}{.55\textwidth}
%		\centering
%		\includegraphics[width=1.1\linewidth]{new_depth_ohsumed_fixed.pdf}
%		\caption{OHSUMED}
%		\label{fig:svm-ohsumed}
%	\end{subfigure}%
%	\begin{subfigure}{.55\textwidth}
%		\centering
%		\includegraphics[width=1.1\linewidth]{new_depth_005_techtc.pdf}
%		\caption{TechTC-100, with feature selection}
%		\label{fig:svm-techtc}
%	\end{subfigure}
%	\caption{Mean SVM Accuracy across all datasets}
%	\label{fig:svm-acc}
%\end{figure}

%Figure \ref{fig:svm-acc} shows the mean accuracy of a SVM classifier for increasing depths. The results for K-NN classifiers are similar in their general trend. For TechTC-100, we see a trend of increasing accuracy, up to a saturation point. This is to be expected, as adding largely orthogonal features would yield improvement until a maximal depth is achieved and no additional features are generated.

%For OHSUMED, we see an interesting effect: an initial increase, followed by a decline in accuracy, and then an increase. This valley-like effect can be due to the amount of generated features. As the search tree depth increases, more features are generated in total. These new features mask existing features and cause a decrease in accuracy. As search continues, powerful local features are located, and accuracy once again increases.
%For the two-level application of FEAGURE, we see a higher accuracy, followed by a slow, constant decline. This decline may be due to the same masking effect, or it can be a similar trend to that of FEAGURE.

%In both cases, we see that the two level application of Deep-FEAGURE yields slightly better accuracy on average.

\subsection{Qualitative Analysis}

TODO

%In order to better understand the contribution of these features, let us look at several of them to try and better understand them. Figure \ref{fig:level1_good} shows one of the features generated for single level feature generation. We can see that the feature attempts to separate based on geographical location. Figure \ref{fig:level1_simple} shows a simple feature regarding people, and showcases the fact that when there is no need for overly complex features, our technique will not force such features. This feature yields a $10\%-15\%$ increase in accuracy (depending on feature selection level), and is generated by both the single and two level approaches.

%Figure \ref{fig:level2_tree} showcases the additional power of the level two approach. The second level learning problem separates non-countries from countries, as well as countries which deal with non fuel manufacturing Arabic nations from those who embargo them.

%Figure \ref{fig:weird_trees} showcases some of the unique oddities of using our technique with the YAGO2 knowledge base. Since YAGO groups together all entities which have a grammatical gender (including fictional characters and non-living objects) under a single relation, the constructed feature looks at both deceased individuals and words that are the same as Australian television channels (such as Gold and Galaxy) at the same time.

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.8 \linewidth]{level1_good.pdf}
%	\caption{A single level feature regarding nations and organizations with official websites}
%	\label{fig:level1_good}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.6 \linewidth]{level1_simple.pdf}
%	\caption{A single level feature regarding people (real or fictional)}
%	\label{fig:level1_simple}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=\linewidth]{level2_tree.pdf}
%	\caption{A two level feature regarding countries and organizations}
%	\label{fig:level2_tree}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.6 \linewidth]{weird_trees.pdf}
%	\caption{A single level feature regarding things with a grammatical gender}
%	\label{fig:weird_trees}
%\end{figure}


\section{Related Work}

Feature generation approaches were found useful for a wide variety of learning problems \cite{markovitch2002feature,ragavan1993complex,utgo1991linear}.
These feature generation approaches search for new features that better represent the target concept than existing features. To that end, there are three major approaches for feature generation: tailored approaches, combinational techniques and methods utilizing external knowledge.
Tailored approaches \cite{sutton1991learning,hirsh1994bootstrapping} are designed for specific problem domains and rely on domain-specific background knowledge for feature construction. One such example is the bootstrapping algorithm \cite{hirsh1994bootstrapping}, which was designed for the domain of molecular biology. The algorithm represents features as nucleotides sequences
whose structure is determined by existing background knowledge. The algorithm uses an initial set of feature sequences, produced by human experts, and uses a domain-specific set of operators to alter them into new sequence features. Such special-purpose algorithms may be effectively tailored for a given domain, but have proven difficult to generalize to other domains and problems

Combinational feature generation techniques aim to locate more descriptive features by combining existing features in various ways, without regards to the specific problem domain. The LDMT algorithm \cite{utgo1991linear}, for example, performs feature construction in the course of building a decision-tree classifier. At each created tree node, the algorithm constructs a hyperplane feature through linear combinations of existing features in a way likely to produce concise, relevant hyperplanes. The FICUS algorithm \cite{markovitch2002feature} describes a general architecture for all combinational feature generation techniques, based on an a feature space specification: a grammar describing the problem domain.
Another major class of combinational feature generation approaches is that of \emph{Deep Learning} \shortcite{lecun1998gradient,bengio2009learning,plotz2011featurefull,kim2013deepfull}. Deep learning techniques seek to create features through complex combinations of existing features, using a deep structure of linear and non-linear functions.
Doing so allows for a wide variety of possible combinational features.
This technique bears some similarities to our approach, in that it allows for the creation of complex features. Unlike deep learning methods, our approach utilizes external knowledge to further improve the generated features. Furthermore, our use of induction algorithms allows for a more generic combination of features.

\subsection{Knowledge based feature generation}

In contrast to combinational approaches, some feature generation algorithms, including FEAGURE, seek to inject additional knowledge into the existing learning problem. These approaches seek to do so almost exclusively through the use of relational data. 
Within this wide family of knowledge-based approaches, we can see a distinction between unsupervised and supervised approaches.
Unsupervised knowledge-based feature generation techniques aim to incorporate additional knowledge from external sources without consideration to the labels supplied by a given training set. They do so in several differing ways:
\begin{itemize}
	\item Concept based approaches such as ESA \cite{gabrilovich2009wikipediafull} ,for example, present a method for generating features that are based on semantic concepts. These approaches rely on complex techniques to create a mapping between existing data and the semantic concepts which serve as external knowledge. This mapping is then used on the given problem, enriching the data with external concepts. ESA, for example, maps texts to Wikipedia pages that are used as concepts.
	\item Propositionalization \cite{kramer2000bottom} approaches are a class of feature generation algorithms that rely on relational data to serve as external knowledge. They operate by using a combination of operators in order to create first-order logic predicates connecting existing data and relational knowledge. 
	\citeA{cheng2011automatedfull} devise a generic framework to do so using linked data via relation-based queries, and offer some insights into taxonomy-based features by creating features that list the taxonomic class relationships of a given entity.
	The \emph{Shallow} algorithm described in section \ref{shallow_section} is another example of propositionalization algorithms.
	\item Data mining approaches such as FeGeLOD \cite{paulheim2012unsupervisedfull} aim to directly utilize linked data in order to automatically enrich existing data, yielding additional information on existing entities within the data. FeGeLOD uses feature values as entities and adds related knowledge to the example, thus creating additional features for that example. FeGeLOD then employs feature selection to filter out poor, non-representative features. This approach differs from our own in that feature values are enriched in an unsupervised manner and in that it attempts to place features of entities directly in the learning problem, in an attempt to supply background information, rather than find a complex concept.
\end{itemize}
These knowledge-based techniques serve as a powerful tool for feature generation based on knowledge. Despite this fact, the unsupervised nature of these approaches tends to lead to a shallow exploration of the knowledge base, as deep unguided search is computationally expensive, and leads to a potentially large number of features. As a result of this, many of these approaches rely on feature selection to filter out irrelevant features.

Unlike unsupervised approaches, some methods, FEAGURE included, seek to utilize a labelled training set in order to perform a more guided search through the space of possible features based on external knowledge. 
These techniques can trace their source to \emph{Inductive Logic Programming (ILP)} \cite{quinlan1990learning,muggleton1991inductive}, a supervised approach that induces a set of first-order logical formulae with the purpose of achieving a good separation of the given training set. ILP methods do so by starting from single relation formulae and adding additional relational constraints using the knowledge base, until formulae that separate the training set into positive and negative examples are found. To that end, these approaches make use of on operator called a refinement operator. When applied on a relational formula, it creates a more specialized case of that formula. For example, given the logical formula $BornIn(X,Y)$, where $X$ is a person and $Y$ is a city, one possible refinement is $BornIn(X,Y)\land CapitalOf(Y,Z)$. The result is a logical formula that considers a more specific case. Additionally, we can look at a refinement that restricts by a constant, turning $BornIn(X,Y)$ into, for example, $BornIn(X, \mbox{{United States}})$.This refinement process continues until a sufficient set of consistent formulas is found.

 \emph{Upgrade} methods such as ICL \cite{van2001upgrade} and SGLR \cite{popescul200716} can be seen as the supervised equivalent of propositionalization methods. Instead of creating first-order predicates a-priori, feature generation is performed during the training phase, allowing for complex features to be considered thanks to a more structured search process that utilizes feature selection based on the target concept.
 While upgrade approaches bear some similarities to our approach, there are several critical differences, the key of which being the ability to more easily locate complex features through the use of existing induction algorithms. 

Another type of supervised, knowledge-based, feature generation approaches is \emph{Relational Learning} techniques. These techniques are designed to utilize relational databases and expand their knowledge. One such technique is that of View Learning \shortcite{davis2005view}. View learning generates new relational tables from the existing relational knowledge, based on labelled data.
It does so by applying ILP on relational data to construct new, simple, relational tables, then using a Bayesian network induction algorithm, learns ways in which these relations can be combined more effectively into a single, more complex table.
This approach bears similarity to our approach in that it can effectively utilize background knowledge to draw conclusions using the combination of labelled data and background knowledge. Unlike our approach, these views attempt to fit new knowledge to existing examples, whereas our approach re-frames the learning problem within a new context.

Kernel-based knowledge injection techniques aim to make use of powerful machine learning algorithms such as SVM and Deep-learning techniques by decoupling the knowledge base from the induction algorithm. This is done through the use of a kernel function that acts as a similarity function between examples in the problem domain. An example of this approach can be seen in the work of \citeA{losch2012graph}, which attempts to learn graph kernels within the relational domain. This creates strong kernel functions that look at the neighbourhood of an entity in search of similarities between entities. This approach bears some similarity to our own in that it can locate complex relationships, but they fundamentally differs in their approach. Kernel methods attempt to create a filter through which entity similarity can be measured. In our approach, we attempt to locate the target concept through the use of different domains. This distinction is especially relevant to classification tasks, where many possible domains can apply for each example, and similarity may be difficult to find when considering the entire example set.

FEAGURE itself is a supervised knowledge-based feature generation approach. It focuses on creation of recursive induction problems, in a manner reminiscent of view learning. This construction allows for a complex exploration of features, which can then be combined in general ways using existing induction methods, in a manner resembling combinational feature generation approaches.
The features of recursive learning problems created by FEAGURE during the feature generation process are similar to those of ILP, relying on a technique that resembles refinement operators. 

\subsection{Knowledge bases and entity linking}

In recent years, we have seen a massive surge in the popularity of relational knowledge bases, starting from the Semantic Web project \cite{bizer2009linkedfull}, through Wikidata \cite{vrandevcic2014wikidata} and more recently, the Google knowledge graph \cite{pelikanova2014google}.

With the increased availability of such knowledge bases, several methods have been devised to make use of them, including several approaches discussed above.
In order to make use of these knowledge bases, one of two conditions must be true:
One possibility is that the problem at hand is already one whose problem domain is entities, such as link prediction \cite{popescul2003statistical2,getoor2005link} and collective classification algorithms \cite{raghavan2004exploration,kajdanowicz2013collectiveaa}. These algorithms assume an existing knowledge base, and attempt to operate within it by either adding links (or relational facts) in the case of the first or by propagating classifications in that of the second.

The other option is to map objects in the problem domain to entities in some way.
To that end, there exist many methods to map objects to entities, ranging from various Entity Recognition \cite<see>{nadeau2007survey} methods, through enriching approaches such as ESA \cite{gabrilovich2009wikipediafull}, to Entity Linking \cite{rao2013entity} and Wikification \cite{bunescu2006using,cheng2013relational}. 
All of the above approaches attempt to map in a variety of ways.
We have already discussed ESA previously, but we note that it has been used to link texts to wikipedia concepts that can be used as entities in many modern relational knowledge bases.

Most Entity Recognition approaches rely on statistical methods combined with linguistic grammar. Through these methods, part-of-speech can be recognized, and likely candidates for entities can be found and cross-referenced with the knowledge base. A better way to link the extracted entities with the ones in the knowledge base is Entity linking. This process seeks to enhance entity recognition by better linking the entities through candidate selection and ranking.
Another approach to this problem is called Wikification. This method seeks to unify the two separate processes of recognition and linking. Similarly to enriching approaches such as ESA, these approaches seek to link words with wikipedia concepts. 

%anything else? still a bit short


\section{Conclusions}
%finishing up and summary

In this paper, we presented FEAGURE, a supervised feature generation algorithm using knowledge bases. Our approach is based around the concept of injecting traditional induction algorithms with external knowledge through the use of features constructed in a supervised manner based on the target concept. 
The algorithm does so through the creation of recursive learning problems based on existing features and the knowledge base, which is then given as input to an induction algorithm. The output of this process is a classifier that is then turned into a feature for the original problem.

In section \ref{formal}, we discussed the feature generation setting, looked at the ways by which an unsupervised approach can be used for feature generation given a complex relational knowledge base, and explained our approach in detail.

In section \ref{text-feagure}, we considered the domain of text categorization as a possible problem for which our approach can be applied, through the use of the bag-of-entities approach. We proceeded to empirically evaluate of our approach on the text categorization learning problem.
This evaluation confirmed that complex knowledge-based feature generation significantly improves classifier accuracy. Experiments on the TechTC-100 dataset show an improvement over a wide range of problems of varying difficulty, with problems considered difficult yielding a very high improvement overall.

The main strength of our approach is its ability to use existing induction approaches in a meaningful way on the knowledge base, allowing for the creation of complex features. These features can then be used by any induction algorithm, giving us a powerful, generic approach.

One limitation of our approach is that even more so than unsupervised feature generation algorithms,  we are dependant on a correct link between feature values and entities within the knowledge base. Mistaken links can lead to incorrect features within the constructed learning problems that may cause the classifier learned to represent a false hypothesis. 

While we focused on applying this approach to text categorization problems, it is important to note that it is applicable to any classification problem where entities can be extracted from feature values, such as drug names, cities and so on. 

\clearpage
\vskip 0.2in
\bibliographystyle{theapa}
\bibliography{document}

\end{document} 