\documentclass[twoside,11pt]{article}
\usepackage{jair,theapa, rawfonts}

% Use the postscript times font!
\usepackage{times}

%my stuff
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{fixltx2e}
\usepackage{dblfloatfix}
\usepackage{subcaption}


%mby
%\usepackage[authoryear]{natbib}
\usepackage[titletoc,toc,title]{appendix}
\usepackage{graphicx}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\jairheading{1}{2015}{3-17}{6/91}{9/91}
\ShortHeadings{Feature Generation by Recursive Induction}
{Friedman \& Markovitch}
%\firstpageno{25}

\title{Deep Feature Generation by Recursive Induction}
\author{\name Lior Friedman \email liorf@cs.technion.ac.il \\
	\name Shaul Markovitch \email shaulm@cs.technion.ac.il \\
	\addr Technion-Israel Institute of Technology\\
	Haifa 32000, Israel
	}

\begin{document}

\maketitle

\begin{abstract}
  Induction algorithms have steadily improved over the years, resulting in powerful methods for learning. However, these methods are constrained to use knowledge within the supplied feature vectors. Recently, a large collection of common-sense and domain specific relational knowledge bases have become available on the web. The natural question is how these knowledge bases can be exploited by existing induction algorithms.
  In this work we propose a novel algorithm for using relational data to generate recursive features. Given a feature, the algorithm recursively defines a new learning task over its set of values, and uses the relational data to construct feature vectors for the new task. The resulting classifier is then used to create the new features.
  We have applied our algorithm to the domain of text categorization, using large semantic knowledge bases such as Freebase. We have shown that generated recursive features significantly improve the performance of existing induction algorithms.
\end{abstract}

\section{Introduction}
\label{sec:Intro}
In recent decades, we have seen an increasing prevalence of machine learning techniques used in a wide variety of fields such as medical diagnosis, vision, and biology.
Most machine learning methods assume a given set of labelled examples, represented by a set of
pre-defined features. These methods have proven to be successful when a collection of good,
distinguishing features is available.
In many real-world applications, however, the given set of features is not sufficient for inducing a high quality classifier.

One approach for overcoming the difficulty resulting from an insufficiently expressive set of features, is to generate new features. There are many existing feature generation algorithms, most of which operating by combining existing features in order to produce new ones. The LFC algorithm \cite{ragavan1993complex} combines the original features using logical operators such as $\land ,\lnot$. LFC generates features using the current set of binary features (including generated ones), and requires that any new feature will yield an increase over the original features combined to create it, in terms of information gain.
The LDMT algorithm \cite{utgo1991linear} uses linear combinations of the original features to construct more informative ones. This allows for the use of non-binary features in feature generation approaches.
The FICUS algorithm \cite{markovitch2002feature} presents a general framework for using any set of constructors to combine features. This essentially generalizes the above approach, allowing for a customizable feature generation framework.

These methods all provide us with ways to enhance the performance of induction algorithms through intelligent combinations of existing features. While this often suffices, there are many cases where merely combining existing features is not sufficient. For this reason, newer approaches aim to incorporate additional knowledge from external sources in order to construct new and informative features.
\citeA{gabrilovich2009wikipediafull} ,for example, present a method for generating features that are based on Wikipedia concepts. They create a mapping of words to Wikipedia articles, that serve as semantic concepts, then utilize the distance of words in the given text to those concepts as features. This approach produced positive results, especially in domains where data is sparse, such as text categorization on short texts.
\citeA{jarmasz2012roget} presents a method for utilizing lexical links between words to generate features. They made use of Roget's Thesaurus as a resource, allowing them to better map words and phrases to their lexical meanings.

In recent years, a new resource in the form of Semantic Linked Data has begun to take form, as part of the Semantic Web project (see survey, \citeR{bizer2009linkedfull}). Semantic Linked Data contains type-annotated entities covering a multitude of domains and connected using multiple semantically meaningful relations.
This resource has led to the creation of several new approaches designed to utilize the new, ontology-based representation of knowledge \cite{losch2012graph,rios2014statistical}.
It should come as no surprise then, that there have been several efforts in utilizing Linked Data for unsupervised feature generation \cite{cheng2011automatedfull,paulheim2012unsupervisedfull}. \citeA{cheng2011automatedfull} devise a theoretical framework for constructing features from linked data. By specifying entity types relevant to the problem at hand, they restrict the space of possible features to a more manageable size, and allow for the creation of a reasonably small amount of features. They also note that this approach tends to lead to highly sparse feature vectors.
\citeA{paulheim2012unsupervisedfull} developed FeGeLOD, an automated, fully unsupervised framework that constructs features by using entity recognition techniques to locate semantically meaningful features in the data set, and expand upon those entities using relations within the Semantic Web. They then use feature selection techniques to remove features with a large percentage of missing, identical, or unique\footnote{Values which only appear in a single example} values. 

Existing feature generation approaches based on Linked Data can offer great benefits, as they can add useful type information, and often add semantic information such as actors playing in a given movie, or the population of a given city.
However, the unsupervised nature of existing approaches limits us greatly when attempting to identify more complex relationships between entities.

In this work, we present a new supervised methodology for generating complex relational features.  Our algorithm constructs new learning problems from existing feature values, using relational data, such as the Semantic Web, as the features for the newly constructed problem.
Using common induction algorithms, we can then construct a classifier that serves as a feature for the original problem. An important aspect of this approach is that it can be applied recursively within the new learning problem, allowing for an automated, data-driven, exploration of the large space of possible  complex features. This allows us to discover powerful features that an unsupervised approach would have difficulty discovering.

\subsection{Illustrative Example} 

Before we delve into the exact description of our algorithm, we would like to showcase its main ideas using an illustrative example.
Suppose we are attempting to identify people with a high risk to be suffering from a certain genetic disease. Assume that the target concept to be discovered is that those at risk are women with ancestors originating from desert areas. To do so, we are given a training sample of sick and healthy people, containing various features, including gender and their full name.
Assuming we have no additional information, a base classifier which can be achieved is the one in figure \ref{fig:tree_base}. While such a classifier will achieve a low training error, the hundreds of seemingly unrelated surnames will cause it to generalize very poorly.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig1.pdf}
	\caption{A decision tree for the basic features}
	\label{fig:tree_base}
\end{figure}

However, if we assume that we have access to a relational knowledge base connecting surnames to common countries of origin, we can begin to apply our feature generation technique to the problem. In the original problem, the node of value female contains objects of type person. Our goal is to separate this set of people to those at high risk and those at low risk. Therefore, Our recursive method defines a new learning problem with the following training set: The objects are surnames; surnames of people with the disease are labelled as positive. The features for these objects are extracted from the knowledge base (See section \ref{algorithm_section}).

Solving the above learning problem through an induction algorithm yields a classifier on surnames. This classifier can be used as a binary feature in the original problem. For example, it can be used as a feature in the node of value female in figure \ref{fig:tree_base}, yielding the tree seen in figure \ref{fig:lvl1_tree}. This new feature gives us a better generalization, as we now abstract the long list of surnames to a short list of countries. This result also allows us to capture previously unseen surnames from those countries. However, this is not a sufficient solution, as we have no way of generalizing on previously unseen countries of origin.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{fig2.pdf}
	\caption{A constructed feature used within a decision tree}
	\label{fig:lvl1_tree}
\end{figure}

Suppose then, that we have access to a knowledge base of facts about countries, such as average temperature, precipitation, and more (one such knowledge base is DBpedia). Using such a knowledge base, we can recursively apply our method while trying to learn the new problem. We create a new training set, the objects of which are countries of origin, and countries of surnames of people with the disease are labelled as positive. Note that this is a second level of recursion. This training set is given to an induction algorithm using the relational knowledge base about countries to construct features. The result is a classifier that tries to separate between countries of origin of people with the disease and those without the disease. The classifier is used as a feature by the first level recursive algorithm. The whole process is depicted in figure \ref{fig:moving_to_lvl2}. The resulting two-level recursive classifier is depicted in figure \ref{fig:lvl2_tree}. This constructed feature allows us to concisely and accurately capture the target concept.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig4_annotated.pdf}
	\caption{Recursive construction of a learning problem on countries of origin. $(1)$-Creating the objects for the new problem. $(2)$-Creating features using the knowledge base. $(3)$-Applying an induction algorithm. $(4)$-The resulting feature.}
	\label{fig:moving_to_lvl2}
\end{figure*}

\begin{figure*}[t]
	\centering
	\includegraphics[width=\linewidth]{fig3.pdf}
	\caption{A two-level constructed feature used within a decision tree}
	\label{fig:lvl2_tree}
\end{figure*}

While this is a simple example, it shows a case where additional information and complex features can result in an overall simpler and more general result. It is important to mention that an unsupervised approach such as FeGeLOD could also solve this problem, but would be forced to generate more features, and utilize complex feature selection to then select relevant features.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Background} \label{background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%One of the earliest methods of utilizing relational information is \emph{Inductive Logic Programming(ILP)} \citep{quinlan1990learning, muggleton1991inductive}. This approach induces a set of first-order formulae that define a good separation of the given training set.
%Following this work, Relational Learning - techniques designed to utilize relational databases, have become increasingly prevalent. One such technique is that of View Learning \citep{davis2005view}, which generated new tables from existing ones, effectively performing feature generation for relational methods.

%One major attempt at adding relational knowledge to traditional induction algorithms was \emph{propositionalization} \citep{kramer2000bottom}: Since we wish to allow the use of first-order predicates in propositional methods, we create possible (non recursive) first-order predicates in a process known as refinement search \citep{van1998completeness}.
%We define a graph of possible first-order formulae by selecting an initial relation and allowing one of two possible refinement operators: The first operator is binding a single variable in the formula by assigning a constant value to it, and the second is binding a single variable using another relation.

%These operators define a search space. For each formula, we can ask whether a given object satisfies it, giving us a binary query for the data. We often prefer to only use formulae with one unbound variable, as it simplifies the satisfaction check.
%It is important to note that each application of refinement operators yields a formula that is subsumed by the parent, meaning that any object satisfying the child formula will also satisfy its parent.

%A major setback of this process is that it generates an impractically large number of features, most of which irrelevant.  To this end, \emph{upgrade} methods such as ICL \citep{van2001upgrade} were suggested, where instead of creating predicates a-priori, we do so during the training phase, allowing us to avoid searching non-promising refinements.

%As a continuation to this trend, \citet{popescul200716} suggest SGLR, an upgrade method which allows the generation of nominal attributes by using aggregation operators. Essentially, instead of simply checking whether a first-order predicate is satisfied, we perform aggregation over all objects which satisfy it. In addition, SGRL offers an initial insight into the issue of improving the search process by using Akaike's information criteria (AIC, see \citet{burnham2002model}) to select features based on a combination of complexity and predictive power.

%In the case where this external knowledge is organized as a set of relations between entities, several methods can be used. One such method is using \emph{Inductive Logic Programming} \citep{muggleton1991inductive} to generate features, in a process referred to as \emph{upgrading} \citep{van2001upgrade}. This is demonstrated through the ICL algorithm, which uses a refinement graph to search for logical formulae which serve as features. The SGLR algorithm \citep{popescul200716} extends this idea to numerical features, generated using aggregation-based techniques, and then utilizes regression for inference.

%TODO: this section to talk about propo as chain of joins? add that needs serious filtering/subsampling to be tractable, and give newish citations (proposizitonaliztion: wordification blah blah paper?)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Using relational knowledge to generate features: A traditional approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Before we discuss our approach, it is prudent to consider existing techniques for turning relational knowledge into additional features.
%Discuss propositionalization?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generating Features through Recursive Induction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $O$ be a set of objects. Let $Y=\{0,1\}$ be a set of labels (we assume binary labels for ease of discussion). Let $S=\{(o_{1},y_{1}),\ldots,(o_{m},y_{m})\}$ be a set of labeled examples such that $o_{i}\in O, y_{i}\in Y$. Let $F=\{f_{1},\ldots,f_{n}\}$ be a \emph{feature map}, a set of \emph{feature functions} $f_{i}:O\rightarrow Dom_{i}$.  This definition implies a training set represented by feature vectors: $N=\{ (\langle f_1(o_i),\ldots,f_n(o_i)\rangle, y_i) | (o_i,y_i) \in S\}$.

Given a set of relations ${\cal R}=\{R_{1},\ldots,R_{t}\}$ with arity of $n_{j}$ ($j=1\ldots t$), we can assume \text{w.l.o.g} that the first argument is a key. For each relation $R_{j}$ we define $n_{j}-1$ new binary relations.
Let $\bar{R}=\{R_{1},\ldots,R_{k}\}$ be such set of binary relations, where $R_{i}$ is defined over $K_{i}\times Dom_{i}$. These relations can thus be seen as functions $R_{i}: K_{i}\rightarrow Dom_{i}$, and therefore we can treat ${\cal R}$ as a set of functions (possibly partial).

\begin{defn}
	A \emph{supervised feature generation algorithm} $A$ using relations is an algorithm that given $\langle S,F,{\cal R} \rangle$, creates a new feature map $F_{{\cal R}}=\{f'_{1},\ldots,f'_{l}\}$.
\end{defn}

We would like the new hypothesis space, defined over $F_{{\cal R}}$, to be one that is both rich enough to provide hypotheses with a lower loss than those in the original space, as well as simple enough that the learning algorithm used will be able to find such good hypotheses given training data.

\subsection{Generating a feature} \label{algorithm_section} 

Given an original feature $f_{i}:O\rightarrow Dom_i$, our algorithm will formulate a new learning task trying to separate values in $Dom_i$ appearing in positive examples of the original learning task from those appearing in negative ones.  The result of the new learning task will be a classifier
$h_{i}:Dom_{i}\rightarrow Y$ that can label feature values of $f_{i}$. We can then define a new feature $f'_{i}(x)=h_{i}(f_{i}(x)), f'_{i}:O\rightarrow Y$.
We name this algorithm \emph{FEAGURE} (FEAture Generation Using REcursive induction). You can see the full pseudo-code in appendix \ref{app:2}.

In order to explain how we create such $h_{i}$, let us consider a single step of the feature generation algorithm.
Given a feature $f_{i}$, we define $v_i(S) = \{v | (o,y) \in S, f_{i}(o)=v\}$ the set of feature values for $f_i$ in $S$. In the intro example, for instance, $v_i(S)$ will be the set of all last names of patients that appeared in the training set.
We now formulate a new learning problem with the new training set
$S'_i = \{ (v, label(v)) | v \in v_i(S) \}$.
The labelling function can be, for example, defined as
the majority label: $label(v)=majority(\{y_k| \left(o_k,y_k \right) \in S, f_{i}(o_k)=v\})$.

To define a feature map over the new training set $S'_{i}$, we look for all relations in ${\cal R}$ where the domain of the key argument contains $v_i$:
$R(v_i,{\cal R}) = \left\{ r \in {\cal R} | v_i(S) \subseteq \{x_1 | (x_1,x_2) \in r\}\right\}$. In the intro example, one such $r$ can be a relation mapping last names to countries of origin. We then use $R(v_i,{\cal R})$ as our feature map.
With this, we have defined a new learning problem over $S'_{i}, R(v_i,{\cal R})$, which yields our classifier $h_{i}:Dom_{i}\rightarrow Y$.
Note that during the process of learning $h_{i}$, we can once again call the feature generation procedure to generate features for the \emph{new} learning problem over $S'_{i}$, hence the recursive aspect of the process. We can see this in the intro example, wherein we construct a second-level problem on countries of origin to correctly identify the appropriate conditions that signify high risk countries.

\begin{algorithm}[H]
	\caption{Creating a recursive problem for a single feature}
	\label{code-creating-prob}
	\small
		%insert param stuff
		\begin{algorithmic}
			\Function{CreateNewProblem}{$f_{i}$, $S$, ${\cal R}$}
                \State $v_i(S) = \{v | (o,y) \in S, f_{i}(o)=v\}$
                \State $S'_i = \{ (v, label(v)) | v \in v_i(S) \}$ 
                \Comment $label(v)$ can be, for example, the majority function.
                \State $R(v_i,{\cal R}) = \left\{ r \in {\cal R} | v_i(S) \subseteq \{x_1 | (x_1,x_2) \in r\}\right\}$
                \State $F_{new}=\{r(v)=c| c\in Range(r), r\in R(v_i,{\cal R})\}$
                \State \Comment These are features from $Dom_{i}$ to $\{0,1\}$. For each relevant relation, we use every possible value.
                \Return $S'_i, F_{new}$ 
                \Comment Our new training set and features define a learning problem.
			\EndFunction
			
		\end{algorithmic}
	\end{algorithm}

We note that while this algorithm deterministically defines a learning problem for a single feature $f_i$, we can use a variety of known machine learning techniques to create $h_i$.
We also note that we can re-apply our approach for this new problem, by simply picking $f_j\in F_{new}$ and applying algorithm \ref{code-creating-prob}.

\subsection{From a Learning Problem to a Feature} \label{why_tree}

Let us discuss how to create $h_i$ from the new learning problem $\langle S'_i,F_{new},R(v,{\cal R})\rangle$.
To begin with, we utilized a Decision Tree classifier \cite{quinlan1986} on the new learning problem. While training the decision tree classifier, we measure information gain for all $f\in F_{new}$, and we can also re-apply our approach as discussed above, creating new features to measure against features in $F_{new}$.

While most traditional machine learning algorithms yield useful properties such as filtering and regularization, a Decision Tree classifier offers several useful traits with regards to recursive application of our approach:
\begin{enumerate}
	\item Decomposability: As we will discuss in section \ref{multi_feature}, a Decision Tree classifier is trivial to decompose into multiple Decision Stumps, allowing us to decide between generating a single strong feature or multiple weaker features.
	\item Interpertability: Decision trees are generally considered easier to interpret and understand, allowing us to easily grasp which relations are more impactful, and thus which domains are likely to contain additional knowledge which we may not have utilized.
	\item Orthogonality: Since features further down in a decision tree are orthogonal to ones already used, we can effectively limit our search space, as relations we have already used will not be picked. This also has a beneficial side effect of increasing variety in generated features.
\end{enumerate}

\subsection{Creating Multiple Features} \label{multi_feature}

Now that we have discussed how we create a single feature, we must consider how to expand this and generate multiple features. One simple approach would be to simply apply the above technique (algorithm \ref{code-creating-prob}) once for each feature.
Naturally, this would result in $n$ new features.
Even if we apply the above approach recursively, we would still obtain only $n$ new features, though they may be stronger as we have generated new features within each learning problem.
Thus, if we wish to generate a larger number of features, we must take a different approach.

Assuming we have utilized a Decision Tree classifier to create $h_i$, we can decompose it to a set of Decision Stumps by taking each decision node in the tree and turning it into a node. By doing so, we can create between $d$ and $2^d$ new features for each $h_i$ ($d$ being the depth of the decision tree). Still, each of these features is weaker than the original $h_i$, since $h_i$ was chosen as a classifier that minimizes training error (subject to regularization) over $S'_i$.

\subsection{Finding Locally Improving Features} \label{tree_usage}

When performing feature generation, we often wish to evaluate the predictive power of generated features. When such evaluation is done in the context of the entire dataset, it is possible to lose the unique benefit of a potentially powerful feature. To address this phenomenon, we decided to apply our recursive feature generation algorithm in a divide-and-conquer approach through the use of a Decision Tree, thus allowing us to better identify features with a strong local impact. In addition to the above, usage of a decision tree yields a second benefit - it allows us to generate features one at a time, creating a more ordered search. It is important to note that the features generated in this process can be later used for \emph{any} induction algorithm.

First, we construct a decision tree node, go over the feature set $F$ and evaluate as normal. We then apply algorithm \ref{code-creating-prob} on all features in $F$, creating our new recursive features. Now, we compare between new and existing features, and pick the best one (We used Information Gain Ratio \cite{quinlan1986}).  
Next, we create son nodes based on feature values, and apply the process recursively until we reach a depth/node size limit in order to avoid over-fitting. 
Finally, the features generated during the decision tree training procedure are collected and returned for use as general, black-box features which can be used along with any induction algorithm. The decision tree itself is discarded.


\begin{algorithm}[H]
	\caption{FEAGURE-FEAture Generation Using REcursive  induction}
	\label{code-tree-thing}
	\small
		minSize- minimal size of a node in the decision tree.

        SplitByFeature- a method that splits a training set according to a feature.
        
		\begin{algorithmic}
			\Function{ConstructFeaturesInANode}{$S$, $F$, ${\cal R}$}
                \State f= \Call{bestFeature}{$S$,$F$} 
                \If {$|S|<$minSize} \Comment{avoids over-fitting}
                    \State
                    \Return \Call{SplitByFeature}{$S$, f}, $\emptyset$
                \EndIf
                \For {$f_i\in F$}
                    \State $S'_i,F_{new}$= \Call{CreateNewProblem}{$f_i$,$S$,${\cal R}$} \Comment{We can apply our algorithm recursively here}
                    \State $h_i$= \Call{InductionAlgorithm}{$S'_i,F_{new}$} 
                    \If {\Call{Compare}{$h_i,f$}$\geq0$} \Comment{for example,information gain ratio}
                        \State add $h_i$ to feature pool $F$
                    \EndIf
                \EndFor
                \State $f_{best}$=\Call{bestFeature}{$S$,$F\cup \{h_i\}$}
                \State let newFeatures be the set of all features added to $F$
                \State \Return \Call{SplitByFeature}{$S$, $f_{best}$}, newFeatures
			\EndFunction

            			
			\State 
            \Function{ConstructFeatures}{$S$, $F$, ${\cal R}$}
                \State currentFeatures= $\emptyset$
                \State sons, $F_{new}$=\Call{ConstructFeaturesInANode}{$S$, $F$, ${\cal R}$}
                \State currentFeatures= currentFeatures$\cup F_{new}$
                \For {son in sons} \Comment{Recursive decision tree training}
                    \State newFeatures=\Call{ConstructFeatures}{son.$S$,$F$,${\cal R}$}
                    \State currentFeatures= currentFeatures$\cup$newFeatures
                \EndFor
                \State \Return currentFeatures
			\EndFunction
		\end{algorithmic}
	\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications for Text Categorization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The text Categorization problem is defined by a set of texts $X$ labelled by a set of categories $Y$ such that we create $S=\{(x_i,y_i)|x_i\in X, y_i\in Y\}$. Given $S$, The learning problem is to create a hypothesis $h:X\rightarrow Y$ which minimizes error (mistaken label).
The standard approach to solving this problem is based on the bag-of-words \cite{Wu:1981:CST:1013228.511759,salton1983introduction} approach, Using the frequencies of word appearances within the text as features and thus learn $h$.

As discussed, we have seen the recent rise of Semantic Linked Data as a powerful knowledge base for text-based entities, with large databases such as Google Knowledge Graph \cite{pelikanova2014google}, Wikidata \cite{vrandevcic2014wikidata} and YAGO2 \cite{hoffart2013yago2}. Naturally, we would like to make use of such semantic data to improve upon the bag-of-words approach, with the implicit assumption that additional Semantic knowledge will allow us to better approximate the relationship between the text and its given label.

Since semantic data utilizes entities, not words and texts as its underlying knowledge components, we must apply some approach which extracts entities from texts. There is a multitude of such methods, such as Named Entity Recognition (NER),  Wikification \cite{bunescu2006using} and Entity Linking \cite{rao2013entity}.
Using the above approaches, we can convert a text $x$ into a set of entities $e(x)$.

\subsection{FEAGURE for Text Categorization}

Given a set of texts converted to entities, we can define $f_i(x)=E(x)$ the entities within the text. We then define ${\cal R}$ to be the set of relations within our knowledge base. Combining these definitions with algorithm \ref{code-creating-prob} we get $v_i(S)=\cup_{x_i}{E(x_i)}$ the set of entities for all texts, $R(v_i,{\cal R})$ is the set of relations in our knowledge base that apply to all entities, and $F_{new}$ would be a set of functions mapping entity keys to their values with regards to relation $r$, so $f_r(x)=\{c|\exists e\in E(x):r(e)=c\}$.

We notice two important difference from the general case that must be discussed.
Firstly, the suggested scheme only considers relations that apply to all entities contained within the text. While such relations exist (for example, the IS-A relation which describes entity type), most relations within a standard knowledge base do not apply to all entity types. To combat this, we allow the use of relations that apply only to some entities (based on domains), allowing us to use all relations within the knowledge base. However, as a result of this it is possible that $f_r(x)=\emptyset$, which will, in turn, mean that the resulting $h$ created during feature generation will be inapplicable to $x$. As a result, $h$ is a partial function $h:X\rightarrow \{0,1\}$, and when using FEAGURE we will have three child nodes: One for value $1$, one for value $0$, and a third for value inapplicable.

The second major difference we must consider is the possibility of having multiple entities for which the resulting classifier applies, potentially yielding multiple classifications for $x$ \footnote{For example, a text mentioning multiple countries, each classified differently by $h$}. To best resolve this, in cases where a feature is applicable to multiple unique words within the text, we return a majority vote of the classification returned by the feature as applied to each such word, as seen in figure \ref{figure5}

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{fig5.pdf}
	\caption{Using a constructed feature in text: $(1)$-Relevant entities are extracted. $(2)$-Each entity is sent to the constructed feature. $(3)$-A majority vote between responses is performed.}
	\label{figure5}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Discussion and Analysis of the FEAGURE algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%tree in outer allows looking at 1 feature a time->more structured search

%We note that in order to prevent cycles which provide no new information, we do not allow the use of any relation that is the inverse of a previously used relation.

%The process of re-labelling and constructing recursive problems as detailed in section \ref{algorithm_section} offers some unique contributions:
%\begin{itemize}
%	\item By moving our domain space when constructing a new problem, we essentially look at the problem from another perspective, which allows for the discovery of complex relationships.
%	\item The process of re-labeling allows noise reduction and emphasizes more general trends within the data that may be harder to otherwise notice.
%	\item We can exploit the power of existing, well-developed learning algorithms when we create a classifier, and possibly use different ones as we take recursive steps.
%\end{itemize}
%Furthermore, we note that FEAGURE can locate locally useful features which may be difficult to identify when looking at the dataset as a whole.

%The resulting features of the FEAGURE algorithm %TODO: discuss that they are classifiers on feature values. also talk on the fact that we expect a few powerful&complex features, rather than many simple ones

%In terms of runtime performance, while FEAGURE has a large overhead, we note two major things: Firstly, the critical section of re-labeling and constructing recursive problems can be parallelized for different recursive problems within a decision tree node.
%Secondly, we note that existing approaches such as the ones described by  \citeA{cheng2011automatedfull} and \citeA{paulheim2012unsupervisedfull}, as well as propositionalization approaches %TODO: citations. possibly this is mentioned earlier?
%perform a chain of join operations ($\Join$) between existing feature values and external knowledge bases, which require similar overheads in terms of performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we discuss our experimental methodology and display our main results.
\subsection{Methodology}

For evaluation, we used two datasets:
\begin{enumerate}
	\item \textbf{TechTC-100} \cite{gabrilovich2004text} - A collection of 100 different binary text categorization problems of varying difficulty, extracted from the Open Dictionary project. On each dataset, we used the Stanford Named Entity Recognizer \cite{finkel2005incorporatingfull} for entity recognition. We then performed stopword elimination using NLTK \cite{bird2009natural} and performed stemming using the Porter Stemmer \cite{van1980new}.
	As our knowledge base, we used \textbf{YAGO2} \cite{hoffart2013yago2}, omitting any relations with literal data such as dates or geographic coordinates. For each relation within YAGO2, we created both the relation itself as well as the inverse relation, and then removed reverse relations that map very few values to very many \footnote{For example, the inverse of the ``has gender" relation  maps ``male" and ``female" to all to all existing people within the YAGO2 database.}.
	
	This dataset collection served as a powerful benchmark, as it contains small multiple text classification problems of varying difficulties. We used YAGO2 as our knowledge base as it contains a great deal of ``common knowledge" - facts regarding countries, events and individuals.
	\item \textbf{OHSUMED} \cite{hersh1994ohsumed} - A dataset of medical abstracts from the MeSH categories of the year 1991. Similarly to \citeA{joachims1998text}, we use only the first 20,000 documents. Furthermore, we limit ourselves to medical documents that only contain a title (that is, there is no available abstract). On each document, stopword elimination and simple stemming was used (due to the medical nature of the texts, the porter stemmer performed poorly). For entity extraction, we used Wikipedia Miner \cite{milne2013open}.  In addition, we only take two categories, C1 and C20 \footnote{Bacterial Infections and Mycoses, Immunologic Diseases} as a binary learning problem. We then applied ten-fold cross validation to rigorously test  our approach.
	As our knowledge base, we used Freebase data dump used by \citeA{bast2014easy}, taking only entities matching our extracted entities. This effectively limits our search space without sacrificing anything in the way of accuracy (unless we apply our algorithm recursively).
	
	This dataset served as a test case representing a larger, more specialized learning problem, where domain-specific knowledge is required. Thus, Freebase which contains scientific facts, including medical facts, was more suitable. We also experimented with a different entity extraction method.
\end{enumerate}

\subsection{Experiment Parameters}

We ran our feature generation algorithm for both a depth of one, creating a recursive learning problem for the original problem, and a depth of two, creating recursive learning problems within a generated learning problem. As discussed in section \ref{multi_feature}, we then flattened the resulting feature trees.
We compared the new set of features (our new features combined with the original features) to the binary bag-of-words features, as well as to a simpler, non-recursive feature generation algorithm, described in algorithm \ref{code-compete}. This algorithm creates a set of features indicating whether a word appears in the text that appears in a certain relation with a specific value. For example, we could generate a feature that checks whether a word in the text appears in relation ``IS-A" with value ``Disease". 

\begin{algorithm}[H]
	\caption{Non-recursive Feature Generation using relations}
	\label{code-compete}
	\small
	%insert param stuff
	\begin{algorithmic}
		\Function{CreateFeaturesForText}{$S$,$F$, ${\cal R}$}
		\State $F_{result}=\emptyset$
		\For {$r \in R$}
		\State  Let $f_{r,c}(x)= \begin{cases} 1 &\mbox{if } \exists f_i\in F :r(f_i(x))=c\\ 
		0 & \mbox{otherwise } \end{cases}$
		\Comment Indicator function for a word appearing in the text such that it is the key of relation $r$ with value $c$
		\State Let $f_r(S)= \cup_{x\in S} f_r(x)=\{c|\exists x\in S,f_i\in F: r(f_i(x))=c\}$
		\Comment All values of $r$ with regards to $F$ (our texts)
		\State  $F_{result}=F_{result}\cup \{f_{r,c}(x)|c\in f_r(S)\}$
		\EndFor
		\State \Return $F_{result}$ 
		\EndFunction
		
	\end{algorithmic}
\end{algorithm}

In order to avoid over-fitting, we limit both tree depth and node size, proportionately to the size of the training set. We also limit these parameters accordingly within the generated recursive problems, as they also utilize a decision tree as their classifier. To decide whether a tree classifier was sufficiently beneficial, we compared its information gain ratio (including the value of inapplicable) to the information gain ratio of the feature with highest information gain ratio. We then allowed a $50\%$ negative penalty parameter to encourage picking generated features assuming they were better than $50\%$ times the best information gain for that decision tree node. 

For labelling objects within constructed learning problems, we use the majority label - The label of an object within the constructed problem will be the label corresponding to the majority of texts containing the entity that was the relation key. Similarly, we use the majority label when deciding on the output of the constructed classifier- In cases where our classifier outputs multiple labels for a single text, we simply take the label corresponding to the majority. We can see an example of this in figure \ref{figure5}.

\subsubsection{Evaluation}

Once we generate our feature set, we proceed to test it by learning a classifier on a training set and measuring its accuracy on a testing set. We made use of three well-known learning algorithms for this task: SVM \cite{cortes1995support}, K-NN \cite{fix1951discriminatory} and CART \cite {breiman1984classification}.
For parameters, for SVM we used a linear kernel and a regularization parameter $C=10$, for K-NN we used $K=3$, and for CART we limited node size according to the size of the training set.

For TechTC-100, we ran our algorithm on all 100 datasets, use the pre-defined training and testing sets to measure accuracy, then ran a paired t-test on the accuracies of our feature set versus both the baseline (no features) and algorithm \ref{code-compete}.
For OHSUMED, we used 10-fold cross-validation to divide our dataset to ten sets of training and testing data. We then proceeded to run our algorithm, measured its accuracy, and ran a paired t-test on the resulting accuracies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{table:acc} shows average accuracies across all 10 folds for OHSUMED, as well as the average accuracies for datasets in techTC-100, using $80\%$ of all available (chosen by highest information gain ratio). Statistical significance over baseline is shown in parenthesis. We also note that results in bold are  significantly better compared to the competing non-recursive approach discussed in algorithm \ref{code-compete}.
For the TechTC-100 dataset, we see a significant improvement over the baseline and competing approaches, even though fewer features are generated than the competing algorithm. We also see that for SVM, the two-level application gives poorer results than the single level FEAGURE algorithm. This can be attributed to over-fitting effects resulting from a combination of mistakes in entity extraction as well as the small dataset size. Due to the nature of linear SVM, such a feature will have significant impact on performance.

Analysis of the results for the OHSUMED datasets show that for K-NN, we do not achieve an improvement over the baseline approach (and neither does the non recursive relation-based algorithm). For SVM, we see a significant improvement over the baseline approach, with a single level application achieving a $2.4\%$ increase in accuracy, and a two-level application giving a total of $2.8\%$ improvement. This is much better than the $1\%$ improvement offered by a non-recursive feature generation algorithm (algorithm \ref{code-compete}). 

\begin{table}[]
	\centering
	\caption{Average Number of Generated Features}
	\label{table:features}
	\begin{tabular}{llll}
		& \# Features(FEAGURE)  & \# Features(FEAGURE 2-level)  & \# Features(Competing) \\
		OHSUMED      & 506.2           & 732        & 27162.2                \\
		TechTC-100  & 63.3       & 65.06      & 5004.66               
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Average accuracy}
	\label{table:acc}
	\begin{tabular}{lllll}
		& Baseline       & Competing & FEAGURE   & FEAGURE 2-level    \\
		OHSUMED KNN    & \textbf{0.777} & 0.756 & 0.769   & 0.75                \\
		OHSUMED SVM    & 0.797 & 0.804   & 0.816    & \textbf{0.819 ($p<.05$)}  \\
		TechTC-100 KNN & 0.527 & 0.523 & 0.533 ($p<.05$) & \textbf{0.557 ($p<.0001$)}  \\
		TechTC-100 SVM & 0.688 & 0.694 & \textbf{0.707 ($p<.0001$)}  & 0.701 ($p<.01$)
	\end{tabular}
\end{table}

%results for svm, results for knn, discuss differences and compete-alg.
Before we look at varying parameters of our algorithm, let us analyze the results on TechTC-100 to better understand the impact of our approach on datasets of varying difficulty. Figure \ref{fig:svm_base_lvl1} shows the accuracies for datasets in techTC-100 using a SVM classifier, The x axis represents the baseline accuracy without feature generation, and the y axis represents the accuracy using our new feature set using FEAGURE. Therefore, any dataset that falls above the $y=x$ line is an improvement in accuracy. We use this visualization technique to illustrate our results on the techTC-100 dataset collection.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{svm_08_base_vs_lvl1}
	\caption{Accuracy of
		baseline approach compared to single activation of FEAGURE (SVM). Each point represents a dataset. The dotted lines represent a 5 and 10 percent difference in accuracy}
	\label{fig:svm_base_lvl1}
\end{figure}

The results for show a general trend of improvement, with high ($> 5\%$) or very high ($>10\%$) improvement being common. We also see multiple datasets where no features are generated, with few datasets showing a degrade in accuracy.
This can be attributed to two major causes:
\begin{itemize}
	\item Errors in the entity extraction process may lead to the creation of misleading entities and thus features. For instance, the word ``One" may be interpreted as the entity ``One (Metallica song)". 
	\item Over-fitting within the feature generation algorithm may create features that appear to have high information gain while generalizing poorly to test data.
\end{itemize}

Figure \ref{fig:25best} shows the 25 hardest datasets in TechTC-100, in terms of lowest accuracy achieved using the full feature set in the original paper of \citeA{gabrilovich2004textresults}. We see that for most of these, we achieve improvement, with a $5\%$ or higher increase being common. We also see several datasets where no features are generated (and thus the accuracy is the same) and three datasets that show a minor degrade in accuracy. This results illustrates that we can, in general, rely on FEAGURE to yield positive recursive features on difficult classification problems.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{25best}
	\caption{Accuracy of
		baseline approach compared to single activation of FEAGURE (SVM). 25 Hardest datasets (lowest accuracy)}
	\label{fig:25best}
\end{figure}

\subsection{Using Non-Tree classifiers with FEAGURE}

As we have discussed in section \ref{algorithm_section}, algorithm \ref{code-creating-prob} creates a generic learning problem. In section \ref{why_tree} we discuss several reasons to use a decision tree on this problem. In this section, we will show the effects of other classifiers on these recursive problems. We have chosen to try both K-NN and SVM, with the following parameters: For K-NN, we used $K=3$. For SVM we used $C=10$, with both Linear and a Radial Basis Function (RBF) kernels. We only construct classifiers for the full problem, and not for any child nodes, unlike algorithm \ref{code-tree-thing}.

Table \ref{table:features-nontree} shows the average number of generated features. We see that these approaches generate far fewer features on average. This should come as no surprise, as we cannot flatten constructed recursive trees in order to generate additional features, nor do we run the learning algorithms on smaller problems.
Table \ref{table:acc-nontree} shows the accuracies achieved by these approaches. We see that while in general, the results are poorer than those achieved by our approach, we do see a clear improvement in accuracy, in many cases granting better accuracy than that achieved by the non-recursive feature generation approach (algorithm \ref{code-compete}).

\begin{table}[]
	\centering
	\caption{Average Number of Generated Features}
	\label{table:features-nontree}
	\begin{tabular}{lllll}
		& \# Features(Tree)  & \# Features(Linear SVM)  & \# Features(RBF SVM) & \# Features(3-NN) \\
		OHSUMED      & 506.2  &  24.7 & 34.5  & 14.6     \\
		TechTC-100  & 63.3   &   12.04    &  8.95   & 4.25        
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Average accuracy}
	\label{table:acc-nontree}
	\begin{tabular}{llllll}
		& Baseline 	& Tree  & Linear SVM & RBF SVM & 3-NN        \\
		OHSUMED KNN    & 0.777  & 0.769  & 0.760 & \textbf{0.795} & 0.756     \\
		OHSUMED SVM    & 0.797    & \textbf{0.816}   & 0.798 & 0.788 & 0.796  \\
		TechTC-100 KNN & 0.527  & \textbf{0.533}  & 0.532  & 0.526 &  0.527 \\
		TechTC-100 SVM & 0.688  & \textbf{0.707}  & 0.695 & 0.692 &  0.696
	\end{tabular}
\end{table}

\subsection{Effect of Constructed Feature Evaluation on Accuracy}

In this section we look at the effect of using a different evaluation method for generated features. In algorithm \ref{code-tree-thing}, we use a comparison function which compares the constructed feature to the best non-relational feature. It may be the case, however, that we should compare our feature in some way to all existing features. One way to do so is to use the mean Information Gain Ratio of features in $F$. Should our new feature score lower than the average, we filter it out. 

\begin{table}[]
	\centering
	\caption{Average Number of Generated Features}
	\label{table:features-average}
	\begin{tabular}{lllll}
		& FEAGURE  & FEAGURE 2-level  & Average IG &  Average IG 2-level \\
		OHSUMED      & 506.2           & 732        &    1270.6   &   1371.4       \\
		TechTC-100  & 63.3       & 65.06      &  234.43 &  244.05             
	\end{tabular}
\end{table}

\begin{table}[]
	\centering
	\caption{Average accuracy}
	\label{table:acc-average}
	\begin{tabular}{llllll}
		& Baseline    & FEAGURE   & FEAGURE 2-level  & Average IG & Average IG 2-level  \\
		OHSUMED KNN    & \textbf{0.777}  & 0.769   & 0.75  & 0.68  & 0.687            \\
		OHSUMED SVM    & 0.797  & 0.816    & \textbf{0.819} & 0.817 & 0.815 \\
		TechTC-100 KNN & 0.527 & 0.533 & 0.557 & 0.557 & \textbf{0.558} \\
		TechTC-100 SVM & 0.688  & 0.707  & 0.701  & \textbf{0.723} & 0.678
	\end{tabular}
\end{table}

Table \ref{table:features-average} shows the amount of features generated by FEAGURE. We see that the amount is significantly greater, indicating that this is a more lax requirement than the one we used (over half of the best information gain ratio). Table \ref{table:acc-average} shows the accuracies attained through this change. For the TechTC-100 dataset collection, we see a general increase in accuracy, especially for single level application. For the OHSUMED dataset, we cannot detect a meaningful improvement, and for K-NN see a degrade in accuracy. 
These results indicate that for small problems, generating additional features is preferable, whereas for problems with many examples, a stronger filtering mechanism is more useful. 


\subsection{Effect of Search Tree Depth on Accuracy}

In this section, we look at the usefulness of searching local problems for additional features.
To do so, we run FEAGURE while limiting the depth of the search tree in algorithm \ref{code-tree-thing}. 
We compare our two datasets side-by-side, without feature selection.

\begin{figure}
	\centering
	\begin{subfigure}{.55\textwidth}
		\centering
		\includegraphics[width=1.1\linewidth]{num_features_ohsumed.pdf}
		\caption{OHSUMED}
		\label{fig:features-depth-ohsumed}
	\end{subfigure}%
	\begin{subfigure}{.55\textwidth}
		\centering
		\includegraphics[width=1.1\linewidth]{num_features_techtc.pdf}
		\caption{TechTC-100}
		\label{fig:features-depth-techtc}
	\end{subfigure}
	\caption{Mean number of features generated}
	\label{fig:features-depth}
\end{figure}

Figure \ref{fig:features-depth} shows the number of generated features per depth.  For both datasets, we see a linear increase, with a two-level application showing a faster increase. In TechTC-100, we see that after a certain depth, there is no additional gain. This is due to the small size of the learning problems.

\begin{figure}
	\centering
	\begin{subfigure}{.55\textwidth}
		\centering
		\includegraphics[width=1.1\linewidth]{svm_ohsumed.pdf}
		\caption{OHSUMED}
		\label{fig:svm-ohsumed}
	\end{subfigure}%
	\begin{subfigure}{.55\textwidth}
		\centering
		\includegraphics[width=1.1\linewidth]{svm_techtc.pdf}
		\caption{TechTC-100}
		\label{fig:svm-techtc}
	\end{subfigure}
	\caption{Mean SVM Accuracy}
	\label{fig:svm-acc}
\end{figure}

Figure \ref{fig:svm-acc} shows the mean accuracy of a SVM classifier for increasing depths. The results for K-NN classifiers are similar their general trend. In TechTC-100, we see a trend of increasing accuracy, up to a saturation point. This is to be expected, as adding largely orthogonal features would yield improvement until a maximal depth is achieved.

For OHSUMED, however, we see a sharp fall in accuracy, followed by a recovery. 
This is caused by the orthogonality of features down the tree combined with errors in entity extraction causing features immediately following the first few tree nodes to separate on mistakenly extracted entities.

%\subsection{Qualitative Analysis}
%In order to better understand the contribution of these features, let us look at several of them to try and better understand them. Figure \ref{fig:level1_good} shows one of the features generated for single level feature generation. We can see that the feature attempts to separate based on geographical location. Figure \ref{fig:level1_simple} shows a simple feature regarding people, and showcases the fact that when there is no need for overly complex features, our technique will not force such features. This feature yields a $10\%-15\%$ increase in accuracy (depending on feature selection level), and is generated by both the single and two level approaches.

%Figure \ref{fig:level2_tree} showcases the additional power of the level two approach. The second level learning problem separates non-countries from countries, as well as countries which deal with non fuel manufacturing Arabic nations from those who embargo them.

%Figure \ref{fig:weird_trees} showcases some of the unique oddities of using our technique with the YAGO2 knowledge base. Since YAGO groups together all entities which have a grammatical gender (including fictional characters and non-living objects) under a single relation, the constructed feature looks at both deceased individuals and words that are the same as Australian television channels (such as Gold and Galaxy) at the same time.

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.8 \linewidth]{level1_good.pdf}
%	\caption{A single level feature regarding nations and organizations with official websites}
%	\label{fig:level1_good}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.6 \linewidth]{level1_simple.pdf}
%	\caption{A single level feature regarding people (real or fictional)}
%	\label{fig:level1_simple}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=\linewidth]{level2_tree.pdf}
%	\caption{A two level feature regarding countries and organizations}
%	\label{fig:level2_tree}
%\end{figure}

%\begin{figure}[]
%	\centering
%	\includegraphics[width=0.6 \linewidth]{weird_trees.pdf}
%	\caption{A single level feature regarding things with a grammatical gender}
%	\label{fig:weird_trees}
%\end{figure}


%TODO: this is not great: too short, propo and upgrade should probably go to the new top part. this should be used to compare to specific papers as well
\section{Related Work}
One of the earliest methods of utilizing relational information is \emph{Inductive Logic Programming(ILP)} \cite{quinlan1990learning,muggleton1991inductive}, which induces a set of first-order formulae that define a good separation of the given training set.
Following this work, Relational Learning - techniques designed to utilize relational databases, have become increasingly prevalent. One such technique is that of View Learning \cite{davis2005view}, which generated new tables from existing ones, effectively performing feature generation for relational methods.
Unlike View Learning and other Relational Learning methods, our approach constructs a new learning problem in a new domain using the existing problem as a guide. Due to this distinction, we essentially try to re-think about our problem from a new direction, rather than trying to fit increasingly less connected entities to the original problem. Furthermore, the ability of our approach to locate locally beneficial features which may be difficult to otherwise detect is invaluable. Finally, we note that the use of decision trees during the feature generation process allows for a natural method by which to filter out features even before applying traditional feature selection mechanisms.

One major attempt at adding relational knowledge to traditional induction algorithms was \emph{propositionalization} \cite{kramer2000bottom}, which allows an unsupervised creations of first-order predicates which can then be used as features for propositional methods. A major setback of this process is that it generates an impractically large number of features, most of which irrelevant.  To this end, \emph{upgrade} methods such as ICL \cite{van2001upgrade} and SGLR \cite{popescul200716} were suggested, where instead of creating predicates a-priori, feature generation is performed during the training phase, in a more structured manner. While upgrade approaches bear some similarities to our approach, there are several critical differences, the key of which is the ability to more easily locate complex features through the use of existing induction algorithms.

Recently, there has been a strong trend of utilizing \emph{Deep Learning} \cite{lecun1998gradient,bengio2009learning} as a feature generation technique. Good examples of this can be seen in FEX \cite{plotz2011featurefull} and DBN2 \cite{kim2013deepfull}. These methods essentially form combinations and transformations on pre-defined features in a semi-supervised manner, thus yielding new, more predictive features. Our approach can be seen as a variation on the same concept, with the transformations being the construction of recursive learning problems, and the combinations being the classifiers constructed in the new problem domain. Our approach differs from those rooted in Deep Learning in both the use of relational data to enrich the feature space, and in that we allow more complex combinations in general.

\section{Conclusions}
%finishing up and summary

We presented a novel new approach to feature generation using relational data, based on constructing and solving new learning problems in the relational domain. 
Our feature generation algorithm constructs powerful and complex features, and can identify locally useful features as well as general trends. These features can be used along any traditional machine learning algorithm.
While we focused on applying this approach to text categorization problems, it is important to note that it is applicable to any classification problem where feature values are categorical and have semantic meanings, such as drug names, cities and so on. Although there is no simple way to generalize this approach to numeric features, we believe there are numerous domains where feature values do have semantic meaning, even excluding text-based domains.
While our approach only generates binary features, aggregation based techniques such as those used in SGLR apply here as well. We furthermore note that when solving non-binary classification problems, we can construct categorical features with, at most, the same amount of values as there are labels in the dataset.

A major source for potential improvement is the subject of matching values to semantic objects. Given the major advances in the field of Wikification \cite{bunescu2006using} in general and Entity Linking \cite{rao2013entity} in particular, these approaches can be used to link the initial text to entities within the semantic data without losing information, which may lead to better results, especially for text classification problems. We have seen in our experiments that primitive entity matching techniques may lead to mistakes which can cascade into poor features.

Another major potential avenue for improvement is to use the labelling techniques discussed section \ref{algorithm_section} as a way to directly label entities within the semantic graph, which yields a Collective Classification \cite{kajdanowicz2013collective} problem in the semantic domain. Solving this learning problem in turn allows us to label any entity within the semantic net, essentially yielding a labelled semantic net trained on our problem. We can then use this net to label new objects within the original problem context through a combination of entity extraction, usage of the semantic labels and label consolidation techniques.

%\clearpage
\vskip 0.2in
\bibliographystyle{theapa}
\bibliography{document}
%\bibliographystyle{plainnat}
%\bibliography{document}

\end{document} 